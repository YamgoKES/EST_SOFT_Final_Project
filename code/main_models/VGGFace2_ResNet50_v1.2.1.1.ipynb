{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c77068",
   "metadata": {},
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ca37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/cydonia999/VGGFace2-pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd VGGFace2-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d18d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchmetrics Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0c3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 부분\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "from functools import partial\n",
    "from models.resnet import resnet50\n",
    "#정확도 및 mae 임포트\n",
    "from torchmetrics import Accuracy, MeanAbsoluteError\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109e7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing(Dataset):\n",
    "    '''\n",
    "    변수 정리\n",
    "\n",
    "    image_dir :이미지 디렉토리\n",
    "    label_dir : 라벨 디렉토리 (json형태 리스트형)\n",
    "    transform : 전처리 및 정규화 (if 문으로 만약 전처리 진행한다면 실행하게..)\n",
    "    categories : 폴더 감정 4개 정의 되어있는 카테고리 뜻함\n",
    "    emotion_return : 감정분석까지 할거면 활용할것\n",
    "    samples : 빈리스트 (채워지는 곳임)\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, image_dir, label_dir, categories, transform=None, emotion_return=False, mode='train') :\n",
    "        self.samples=[]\n",
    "        self.transform = transform\n",
    "        self.label_map= { cat : idx for idx, cat in enumerate(categories)}\n",
    "        self.emotion_return = emotion_return\n",
    "        self.mode = mode\n",
    "        \n",
    "        # 나이 정규화 위한 최소/최대 나이 정의 (예: 20~60)\n",
    "        self.age_min = 10\n",
    "        self.age_max = 60\n",
    "\n",
    "        for category in categories :\n",
    "            json_path =os.path.join(label_dir, f'{self.mode}_{category}.json')\n",
    "            img_folder = os.path.join(image_dir,category)\n",
    "\n",
    "            with open(json_path,'r',encoding='euc-kr') as f:\n",
    "                label_data = json.load(f)\n",
    "\n",
    "            for row in label_data :\n",
    "                filename = row['filename']\n",
    "\n",
    "                img_path = os.path.join(img_folder,filename)\n",
    "\n",
    "                if os.path.isfile(img_path) :\n",
    "                    \n",
    "                    age = row.get('age')\n",
    "                    \n",
    "                    # 나이 정규화 (0~1 사이)\n",
    "                    if age is not None:\n",
    "                        age_norm = (age - self.age_min) / (self.age_max - self.age_min)\n",
    "                    else:\n",
    "                        age_norm = 0.0  # 결측값 처리 예시\n",
    "                        \n",
    "                    sample = {\n",
    "                        'img_path' : img_path,\n",
    "                        'category' : category,\n",
    "                        'age' : age_norm,\n",
    "                        'gender' : row.get('gender')\n",
    "                    }\n",
    "                    self.samples.append(sample)\n",
    "\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "    def __getitem__(self,idx) :\n",
    "        sample = self.samples[idx]\n",
    "        image = Image.open(sample['img_path']).convert('RGB')\n",
    "\n",
    "        if self.transform :\n",
    "            image = self.transform(image)\n",
    "\n",
    "\n",
    "        #emotion_label = torch.tensor(self.label_map[sample['category']], dtype=torch.long) 수정1\n",
    "        age=torch.tensor(sample['age'], dtype=torch.float32)\n",
    "        gender = torch.tensor(1 if sample['gender']=='남' else 0, dtype=torch.long)\n",
    "\n",
    "        # return image, emotion_label, age, gender 수정1\n",
    "\n",
    "        if self.emotion_return :\n",
    "            emotion_label = torch.tensor(self.label_map[sample['category']], dtype=torch.long)\n",
    "            return image, emotion_label, age, gender\n",
    "\n",
    "        else :\n",
    "            return image, age, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "620a2f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform (개인 커스텀 할 것을 추천드림)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)), #이미지 사이즈 조정\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456,0.406], #RGB평균\n",
    "                         [0.229,0.224,0.225])  #RGB 표준편차\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 업로드  랜덤으로 합시다 checking용\n",
    "categories=['anger','happy','panic','sadness']\n",
    "\n",
    "#상대경로이므로 꼭 맞춰서 다운받을것\n",
    "base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "train_image_dir = os.path.join(base_dir,'Data','img', 'train')\n",
    "train_label_dir = os.path.join(base_dir,'Data','label', 'train')\n",
    "val_image_dir = os.path.join(base_dir,'Data','img', 'val')\n",
    "val_label_dir = os.path.join(base_dir,'Data','label', 'val')\n",
    "\n",
    "#임의 추출\n",
    "# def sample_from_dataset(dataset, categories, num_per_category=50) :\n",
    "#     category_indices={cat: [] for cat in categories}\n",
    "\n",
    "#     for idx, sample in enumerate(dataset.samples) :\n",
    "#         category=sample['category']\n",
    "#         category_indices[category].append(idx)\n",
    "\n",
    "#     selected_indices = []\n",
    "\n",
    "#     for cat in categories :\n",
    "#         indices =category_indices[cat]\n",
    "#         sampled =random.sample(indices,min(num_per_category, len(indices)))\n",
    "#         selected_indices.extend(sampled)\n",
    "\n",
    "#     return selected_indices\n",
    "\n",
    "# 카테고리별 랜덤 100장씩 추출 (train)\n",
    "train_data_load=DataProcessing(train_image_dir,train_label_dir,categories,transform=transform, mode='train')\n",
    "#train_selected_data = sample_from_dataset(train_data_load,categories, num_per_category=200)\n",
    "\n",
    "#val에서 50장 추출\n",
    "val_data_load = DataProcessing(val_image_dir, val_label_dir,categories, transform=transform, mode='val')\n",
    "#val_selected_data = sample_from_dataset(val_data_load, categories, num_per_category=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7b1041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================= 이부분은 나중에 전체 학습할거라 지워도 무방한곳 확인용으로 만든 코드 =========================================\n",
    "#train_subset=torch.utils.data.Subset(train_data_load, train_selected_data) #100개 랜덤 뽑기 *4\n",
    "train_loader= DataLoader(train_data_load, batch_size=32, shuffle=True)\n",
    "\n",
    "#val_subset=torch.utils.data.Subset(val_data_load, val_selected_data) #50개 랜덤 뽑기*4\n",
    "val_loader= DataLoader(val_data_load, batch_size=32, shuffle=True)\n",
    "#========================================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3814c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2e87017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================모델가져오기================\n",
    "# model = resnet50()\n",
    "\n",
    "# # 2. 마지막 fc 레이어 바꾸기 (튜닝용 구조)\n",
    "# model.fc = nn.Sequential(\n",
    "#     nn.Linear(model.fc.in_features, 256),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.Linear(256, 3)  # age + gender (2 classes)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1feabc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #====================모델 가져오기 2번째 custum v1.1 ==============\n",
    "# model_v1_1 =resnet50()\n",
    "\n",
    "# model_v1_1.fc = nn.Sequential(\n",
    "#     nn.Linear(model_v1_1.fc.in_features,256),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.Linear(256,128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.2),\n",
    "#     nn.Linear(128,64),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.1),\n",
    "#     nn.Linear(64,3)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48bf4ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================모델 가져오기 2번째 custum v1.2 ==============\n",
    "model_v1_2_1_1 = resnet50()\n",
    "\n",
    "model_v1_2_1_1.fc = nn.Sequential(\n",
    "    nn.Linear(model_v1_2_1_1.fc.in_features,256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    \n",
    "    nn.Linear(256,128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    \n",
    "    nn.Linear(128,64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    \n",
    "    nn.Linear(64,3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ 3.가중치 불러오기 ===============\n",
    "base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "weight_path = os.path.join(base_dir, 'resnet50_ft_weight.pkl')\n",
    "\n",
    "with open(weight_path, 'rb') as f:\n",
    "    state_dict = pickle.load(f)\n",
    "    \n",
    "\n",
    "for key in state_dict:\n",
    "    if isinstance(state_dict[key], np.ndarray):\n",
    "        state_dict[key] = torch.from_numpy(state_dict[key])\n",
    "\n",
    "model_v1_2_1_1.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\n",
    "# 5. 디바이스에 올리기\n",
    "model_v1_2_1_1 = model_v1_2_1_1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "687f64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================\n",
    "criterion_age = nn.MSELoss()\n",
    "criterion_gender = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_v1_2_1_1.parameters(), lr=1e-4)\n",
    "num_epochs = 10 #수정가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "936963a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, criterion_age, criterion_gender) :\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    accuracy = Accuracy(task='binary').to(device)\n",
    "    mae = MeanAbsoluteError().to(device) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, ages, genders in data_loader :\n",
    "            images = images.to(device)\n",
    "            ages  = ages.to(device)\n",
    "            genders = genders.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predicted_age = outputs[:,0]\n",
    "            predicted_gender_logits = outputs[:,1:3]\n",
    "            \n",
    "            loss_age = criterion_age(predicted_age, ages)\n",
    "            loss_gender = criterion_gender(predicted_gender_logits, genders)\n",
    "            loss = loss_age + loss_gender\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred = torch.argmax(predicted_gender_logits, dim=1)\n",
    "            accuracy.update(pred, genders)\n",
    "            mae.update(predicted_age, ages)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)        \n",
    "    return avg_loss, accuracy.compute(), mae.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af4fa7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============모델 저장을 위한 빈 리스트 생성=============\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_maes = []\n",
    "val_maes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58c6befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================Early Stopping======================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False):\n",
    "        self.patience = patience      # 개선 없을 때 참을 에폭 수\n",
    "        self.verbose = verbose        # 멈출 때 출력 여부\n",
    "        self.counter = 0              # 개선 없을 때 카운트\n",
    "        self.best_loss = np.Inf       # 최저 검증 손실 저장\n",
    "        self.early_stop = False       # 멈춤 여부\n",
    "        self.best_model_state = None  # 최적 모델 가중치 저장\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss improved to {val_loss:.4f}. Saving model.')\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'No improvement for {self.counter} epochs.')\n",
    "            if self.counter >= self.patience:\n",
    "                if self.verbose:\n",
    "                    print('Early stopping triggered.')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ce1c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Train Loss :  0.3911, Train Gender Accuracy :  0.9161, Train MAE :  0.2760\n",
      "Validation Loss :  0.4166, Validation Gender Accuracy :  0.8950, Validation MAE :  0.2724\n",
      "Validation loss improved to 0.4166. Saving model.\n",
      "Epoch [2/10]\n",
      "Train Loss :  0.1940, Train Gender Accuracy :  0.9633, Train MAE :  0.1380\n",
      "Validation Loss :  0.2633, Validation Gender Accuracy :  0.9183, Validation MAE :  0.1364\n",
      "Validation loss improved to 0.2633. Saving model.\n",
      "Epoch [3/10]\n",
      "Train Loss :  0.1324, Train Gender Accuracy :  0.9745, Train MAE :  0.1362\n",
      "Validation Loss :  0.2275, Validation Gender Accuracy :  0.9275, Validation MAE :  0.1298\n",
      "Validation loss improved to 0.2275. Saving model.\n",
      "Epoch [4/10]\n",
      "Train Loss :  0.1030, Train Gender Accuracy :  0.9862, Train MAE :  0.1312\n",
      "Validation Loss :  0.2209, Validation Gender Accuracy :  0.9258, Validation MAE :  0.1284\n",
      "Validation loss improved to 0.2209. Saving model.\n",
      "Epoch [5/10]\n",
      "Train Loss :  0.0702, Train Gender Accuracy :  0.9898, Train MAE :  0.1342\n",
      "Validation Loss :  0.1699, Validation Gender Accuracy :  0.9425, Validation MAE :  0.1296\n",
      "Validation loss improved to 0.1699. Saving model.\n",
      "Epoch [6/10]\n",
      "Train Loss :  0.0759, Train Gender Accuracy :  0.9880, Train MAE :  0.1318\n",
      "Validation Loss :  0.2416, Validation Gender Accuracy :  0.9300, Validation MAE :  0.1283\n",
      "No improvement for 1 epochs.\n",
      "Epoch [7/10]\n",
      "Train Loss :  0.0494, Train Gender Accuracy :  0.9968, Train MAE :  0.1297\n",
      "Validation Loss :  0.1419, Validation Gender Accuracy :  0.9583, Validation MAE :  0.1235\n",
      "Validation loss improved to 0.1419. Saving model.\n",
      "Epoch [8/10]\n",
      "Train Loss :  0.0604, Train Gender Accuracy :  0.9920, Train MAE :  0.1292\n",
      "Validation Loss :  0.1841, Validation Gender Accuracy :  0.9450, Validation MAE :  0.1260\n",
      "No improvement for 1 epochs.\n",
      "Epoch [9/10]\n",
      "Train Loss :  0.0719, Train Gender Accuracy :  0.9878, Train MAE :  0.1310\n",
      "Validation Loss :  0.1993, Validation Gender Accuracy :  0.9417, Validation MAE :  0.1268\n",
      "No improvement for 2 epochs.\n",
      "Epoch [10/10]\n",
      "Train Loss :  0.0455, Train Gender Accuracy :  0.9945, Train MAE :  0.1300\n",
      "Validation Loss :  0.1748, Validation Gender Accuracy :  0.9483, Validation MAE :  0.1255\n",
      "No improvement for 3 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#===================== 학습 ===========================\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_v1_2_1_1.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for images, ages, genders in train_loader:\n",
    "        \n",
    "        images = images.to(device)\n",
    "        ages = ages.to(device)\n",
    "        genders = genders.to(device)\n",
    "\n",
    "        outputs = model_v1_2_1_1(images)\n",
    "        predicted_age = outputs[:, 0]\n",
    "        predicted_gender_logits = outputs[:, 1:3]\n",
    "\n",
    "        loss_age = criterion_age(predicted_age, ages)\n",
    "        loss_gender = criterion_gender(predicted_gender_logits, genders)\n",
    "        loss = loss_age + loss_gender\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    #avg_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    # === 평가 ===\n",
    "    train_loss, train_acc, train_mae = evaluate(model_v1_2_1_1, train_loader, device, criterion_age, criterion_gender)\n",
    "    val_loss, val_acc, val_mae = evaluate(model_v1_2_1_1, val_loader, device, criterion_age, criterion_gender)\n",
    "\n",
    "    # === 저장 ===\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc.item())\n",
    "    val_accuracies.append(val_acc.item())\n",
    "    train_maes.append(train_mae.item())\n",
    "    val_maes.append(val_mae.item())\n",
    "\n",
    "    # ==== 출력 =====\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'Train Loss : {train_loss : .4f}, Train Gender Accuracy : {train_acc : .4f}, Train MAE : {train_mae : .4f}')\n",
    "    print(f'Validation Loss : {val_loss : .4f}, Validation Gender Accuracy : {val_acc : .4f}, Validation MAE : {val_mae : .4f}')\n",
    "    \n",
    "    early_stopping(val_loss, model_v1_2_1_1)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# 가장 좋은 가중치로 복원\n",
    "model_v1_2_1_1.load_state_dict(early_stopping.best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "pth_save_path = os.path.join(base_dir, 'pth_pkl', 'model_raw_weights_v1_2_1_1.pth')\n",
    "\n",
    "try:\n",
    "    torch.save(model_v1_2_1_1.state_dict(), pth_save_path)\n",
    "    print(f'모델 저장 완료 → {pth_save_path}')\n",
    "except Exception as e:\n",
    "    print(f'모델 저장 실패: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a45b0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'train_maes': train_maes,\n",
    "    'val_maes': val_maes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "pkl_save_path = os.path.join(base_dir, 'pth_pkl', 'model_raw_v1_2_1_1.pkl')\n",
    "\n",
    "try:\n",
    "    with open(pkl_save_path, \"wb\") as f:\n",
    "        pickle.dump(history, f)\n",
    "    print(f'학습 기록이 성공적으로 저장되었습니다 : {pkl_save_path}')\n",
    "except Exception as e:\n",
    "    print(f'학습 기록 저장 실패: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple overfit checking \n",
    "\n",
    "if train_loss>val_loss or train_acc < val_acc :\n",
    "    print('overfit예상')\n",
    "\n",
    "elif train_mae > val_mae :\n",
    "    print('과소적합 에상')\n",
    "\n",
    "else:\n",
    "    print('적절하게 학습되었을 가능성 높음')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef937a8",
   "metadata": {},
   "source": [
    "정규화 및 Dropout 강화<br>\n",
    "\n",
    "Dropout 비율 올리기, BatchNorm 층 추가<br>\n",
    "\n",
    "데이터 증강(Augmentation)<br>\n",
    "\n",
    "훈련 이미지 다양화<br>\n",
    "\n",
    "더 많은 데이터 사용 또는 더 적은 복잡도 모델<br>\n",
    "\n",
    "훈련 데이터가 적다면 데이터 확보<br>\n",
    "\n",
    "조기 종료(Early Stopping) 도입<br>\n",
    "\n",
    "검증 성능 안 좋아질 때 학습 중단<br>\n",
    "\n",
    "학습률 감소 (Learning rate scheduling)<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
