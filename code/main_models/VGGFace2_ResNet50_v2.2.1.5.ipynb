{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51bd66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/cydonia999/VGGFace2-pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e95a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd VGGFace2-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5f73cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 부분\n",
    "import time\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "from functools import partial\n",
    "from models.resnet import resnet50\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "#정확도 및 mae 임포트\n",
    "from torchmetrics import Accuracy, MeanAbsoluteError\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafae947",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"CUDA 사용 가능:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"현재 CUDA 디바이스 인덱스:\", torch.cuda.current_device())\n",
    "    print(\"CUDA 디바이스 이름:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA를 사용할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "042923e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"사용 디바이스:\", device)\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"현재 CUDA 디바이스 인덱스:\", torch.cuda.current_device())\n",
    "    print(\"CUDA 디바이스 이름:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2c51e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing(Dataset) :\n",
    "    \n",
    "    def __init__(self, image_dir, label_dir, categories, transform=None, mode='train') :\n",
    "        \n",
    "        self.datalist = []\n",
    "        self.transform = transform\n",
    "        self.label_map = {cat : idx for idx, cat in enumerate(categories)} #숫자로 변환 ?\n",
    "        self.mode = mode \n",
    "        self.age_min = 10\n",
    "        self.age_max = 60\n",
    "        \n",
    "        for category in categories :\n",
    "            \n",
    "            json_path = os.path.join(label_dir, f'{self.mode}_crop_{category}.json') \n",
    "            img_folder =  os.path.join(image_dir, category)\n",
    "            \n",
    "            with open(json_path, 'r', encoding='utf-8') as f :\n",
    "                label_data = json.load(f)        \n",
    "                \n",
    "            for row in label_data :\n",
    "                filename = row['filename']  # 예: 'abc_crop20.jpg'\n",
    "                \n",
    "                # base_filename은 확장자(.jpg)만 제거한 원본명\n",
    "                base_filename = filename.replace('.jpg', '')  \n",
    "                \n",
    "                # base_filename으로 시작하는 모든 jpg파일(원본+증강) 찾기\n",
    "                matched_files = [f for f in os.listdir(img_folder) if f.startswith(base_filename) and f.endswith('.jpg')]\n",
    "                \n",
    "                for matched_file in matched_files:\n",
    "                    img_path = os.path.join(img_folder, matched_file)\n",
    "\n",
    "                    if not os.path.isfile(img_path):\n",
    "                        continue\n",
    "\n",
    "                    age = row.get('age')\n",
    "                    if age is not None and age >= 60:\n",
    "                        continue\n",
    "\n",
    "                    age_norm = (age - self.age_min) / (self.age_max - self.age_min) if age is not None else 0.0\n",
    "\n",
    "                    data = {\n",
    "                        'img_path': img_path,\n",
    "                        'category': category,\n",
    "                        'age': age_norm,\n",
    "                        'raw_age': age,\n",
    "                        'gender': row.get('gender'),\n",
    "                    }\n",
    "                    self.datalist.append(data)\n",
    "                \n",
    "    def __len__(self) :\n",
    "        return len(self.datalist)\n",
    "    \n",
    "    def __getitem__(self,idx) :\n",
    "        \n",
    "        data_item = self.datalist[idx]\n",
    "        image = Image.open(data_item['img_path']).convert('RGB')\n",
    "        \n",
    "        if self.transform is not None :\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        \n",
    "        \n",
    "        age=torch.tensor(data_item['age'], dtype=torch.float32)\n",
    "        gender = torch.tensor(1 if data_item['gender']=='남' else 0, dtype=torch.long)\n",
    "\n",
    "        return image, age, gender        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling_weights(dataset):\n",
    "    weights = []\n",
    "    for sample in dataset.datalist:\n",
    "        age = sample.get('raw_age')\n",
    "        gender = sample.get('gender')\n",
    "\n",
    "        if age is None:\n",
    "            weights.append(1.0)\n",
    "            continue\n",
    "\n",
    "        age_group = (age // 10) * 10  # 10대, 20대, ...\n",
    "        if age_group == 10:\n",
    "            weight = 3.0 if gender == '남' else 1.3\n",
    "        elif age_group == 40:\n",
    "            weight = 1.1\n",
    "        elif age_group == 50:\n",
    "            weight = 2.0 if gender == '남' else 1.5\n",
    "        elif age_group ==20 :\n",
    "            weight = 1.4 if gender == '남' else 1.2\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        weights.append(weight)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ca12ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456,0.406], #RGB평균\n",
    "                         [0.229,0.224,0.225])  #RGB 표준편차\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categories=['anger','happy','panic','sadness']\n",
    "\n",
    "base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "train_image_dir = os.path.join(base_dir, 'augment')  # Final/augment\n",
    "train_label_dir = os.path.join(base_dir, 'CropData2', 'label', 'train')  # Final/CropData2/label/train\n",
    "\n",
    "val_image_dir = os.path.join(base_dir, 'CropData2', 'img', 'val')  # Final/CropData2/img/val\n",
    "val_label_dir = os.path.join(base_dir, 'CropData2', 'label', 'val')  # Final/CropData2/label/val\n",
    "\n",
    "train_data_load=DataProcessing(train_image_dir,train_label_dir,categories,transform=transform, mode='train')\n",
    "val_data_load = DataProcessing(val_image_dir, val_label_dir,categories, transform=transform, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = get_sampling_weights(train_data_load)\n",
    "\n",
    "# 샘플러 정의\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data_load,\n",
    "    batch_size=64,\n",
    "    sampler=sampler,\n",
    "    #num_workers=4,   # 2->4로 올려보기\n",
    "    pin_memory=True  # GPU 사용 시 권장\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_data_load,\n",
    "    batch_size=64,\n",
    "    shuffle=False,     # 검증은 보통 셔플 안함\n",
    "    #num_workers=4,     # 워커 4개로 증가\n",
    "    pin_memory=True    # GPU에 최적화\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a181a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================모델 가져오기 2번째 custum v1.2 ==============\n",
    "model_v2_2_1_5 = resnet50()\n",
    "\n",
    "model_v2_2_1_5.fc = nn.Sequential(\n",
    "    nn.Linear(model_v2_2_1_5.fc.in_features,256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    \n",
    "    nn.Linear(256,128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    \n",
    "    nn.Linear(128,64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    \n",
    "    nn.Linear(64,3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb502c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================ 3.가중치 불러오기 ===============\n",
    "base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "weight_path = os.path.join(base_dir, 'resnet50_ft_weight.pkl')\n",
    "\n",
    "with open(weight_path, 'rb') as f:\n",
    "    state_dict = pickle.load(f)\n",
    "    \n",
    "\n",
    "for key in state_dict:\n",
    "    if isinstance(state_dict[key], np.ndarray):\n",
    "        state_dict[key] = torch.from_numpy(state_dict[key])\n",
    "\n",
    "model_v2_2_1_5.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\n",
    "# 5. 디바이스에 올리기\n",
    "model_v2_2_1_5 = model_v2_2_1_5.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================\n",
    "criterion_age = nn.MSELoss()\n",
    "criterion_gender = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_v2_2_1_5.parameters(), lr=1e-4)\n",
    "num_epochs = 30 #수정가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6895508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, criterion_age, criterion_gender) :\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    accuracy = Accuracy(task='binary').to(device)\n",
    "    mae = MeanAbsoluteError().to(device) \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, ages, genders in data_loader :\n",
    "            images = images.to(device)\n",
    "            ages  = ages.to(device)\n",
    "            genders = genders.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predicted_age = outputs[:,0]\n",
    "            predicted_gender_logits = outputs[:,1:3]\n",
    "            \n",
    "            loss_age = criterion_age(predicted_age, ages)\n",
    "            loss_gender = criterion_gender(predicted_gender_logits, genders)\n",
    "            loss = loss_age + loss_gender\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred = torch.argmax(predicted_gender_logits, dim=1)\n",
    "            accuracy.update(pred, genders)\n",
    "            mae.update(predicted_age, ages)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)        \n",
    "    return avg_loss, accuracy.compute(), mae.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c12b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============모델 저장을 위한 빈 리스트 생성=============\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_maes = []\n",
    "val_maes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db575dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================Early Stopping======================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False):\n",
    "        self.patience = patience      # 개선 없을 때 참을 에폭 수\n",
    "        self.verbose = verbose        # 멈출 때 출력 여부\n",
    "        self.counter = 0              # 개선 없을 때 카운트\n",
    "        self.best_loss = np.Inf       # 최저 검증 손실 저장\n",
    "        self.early_stop = False       # 멈춤 여부\n",
    "        self.best_model_state = None  # 최적 모델 가중치 저장\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss improved to {val_loss:.4f}. Saving model.')\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'No improvement for {self.counter} epochs.')\n",
    "            if self.counter >= self.patience:\n",
    "                if self.verbose:\n",
    "                    print('Early stopping triggered.')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060f590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] - Training loss: 0.5685\n",
      "Epoch [2/30] - Training loss: 0.2000\n",
      "Epoch [3/30]\n",
      "Train Loss : 0.0564, Train Gender Accuracy : 0.9956, Train AGE MAE : 0.1029\n",
      "Validation Loss : 0.0692, Validation Gender Accuracy : 0.9900, Validation AGE MAE : 0.0996\n",
      "Validation loss improved to 0.0692. Saving model.\n",
      "Epoch [4/30] - Training loss: 0.1043\n",
      "Epoch [5/30] - Training loss: 0.0898\n",
      "Epoch [6/30]\n",
      "Train Loss : 0.0363, Train Gender Accuracy : 0.9960, Train AGE MAE : 0.0949\n",
      "Validation Loss : 0.0581, Validation Gender Accuracy : 0.9875, Validation AGE MAE : 0.0881\n",
      "Validation loss improved to 0.0581. Saving model.\n",
      "Epoch [7/30] - Training loss: 0.0680\n",
      "Epoch [8/30] - Training loss: 0.0636\n",
      "Epoch [9/30]\n",
      "Train Loss : 0.0275, Train Gender Accuracy : 0.9964, Train AGE MAE : 0.0797\n",
      "Validation Loss : 0.0545, Validation Gender Accuracy : 0.9859, Validation AGE MAE : 0.0884\n",
      "Validation loss improved to 0.0545. Saving model.\n",
      "Epoch [10/30] - Training loss: 0.0591\n",
      "Epoch [11/30] - Training loss: 0.0470\n",
      "Epoch [12/30]\n",
      "Train Loss : 0.0381, Train Gender Accuracy : 0.9962, Train AGE MAE : 0.1175\n",
      "Validation Loss : 0.0673, Validation Gender Accuracy : 0.9842, Validation AGE MAE : 0.1121\n",
      "No improvement for 1 epochs.\n",
      "Epoch [13/30] - Training loss: 0.0429\n",
      "Epoch [14/30] - Training loss: 0.0424\n",
      "Epoch [15/30]\n",
      "Train Loss : 0.0196, Train Gender Accuracy : 0.9983, Train AGE MAE : 0.0873\n",
      "Validation Loss : 0.0557, Validation Gender Accuracy : 0.9884, Validation AGE MAE : 0.0857\n",
      "No improvement for 2 epochs.\n",
      "Epoch [16/30] - Training loss: 0.0334\n",
      "Epoch [17/30] - Training loss: 0.0374\n",
      "Epoch [18/30]\n",
      "Train Loss : 0.0221, Train Gender Accuracy : 0.9969, Train AGE MAE : 0.0777\n",
      "Validation Loss : 0.0573, Validation Gender Accuracy : 0.9859, Validation AGE MAE : 0.0862\n",
      "No improvement for 3 epochs.\n",
      "Epoch [19/30] - Training loss: 0.0345\n",
      "Epoch [20/30] - Training loss: 0.0301\n",
      "Epoch [21/30]\n",
      "Train Loss : 0.0077, Train Gender Accuracy : 0.9998, Train AGE MAE : 0.0577\n",
      "Validation Loss : 0.0606, Validation Gender Accuracy : 0.9867, Validation AGE MAE : 0.0796\n",
      "No improvement for 4 epochs.\n",
      "Epoch [22/30] - Training loss: 0.0294\n",
      "Epoch [23/30] - Training loss: 0.0227\n",
      "Epoch [24/30]\n",
      "Train Loss : 0.0631, Train Gender Accuracy : 0.9883, Train AGE MAE : 0.1470\n",
      "Validation Loss : 0.1510, Validation Gender Accuracy : 0.9651, Validation AGE MAE : 0.1543\n",
      "No improvement for 5 epochs.\n",
      "Early stopping triggered.\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_v2_2_1_5.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for images, ages, genders in train_loader:\n",
    "        images = images.to(device)\n",
    "        ages = ages.to(device)\n",
    "        genders = genders.to(device)\n",
    "\n",
    "        outputs = model_v2_2_1_5(images)\n",
    "        predicted_age = outputs[:, 0]\n",
    "        predicted_gender_logits = outputs[:, 1:3]\n",
    "\n",
    "        loss_age = criterion_age(predicted_age, ages)\n",
    "        loss_gender = criterion_gender(predicted_gender_logits, genders)\n",
    "        loss = loss_age + loss_gender\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # 3 에폭마다 평가 및 기록 수행\n",
    "    if (epoch + 1) % 3 == 0 or epoch == num_epochs - 1:\n",
    "        train_loss, train_acc, train_mae = evaluate(model_v2_2_1_5, train_loader, device, criterion_age, criterion_gender)\n",
    "        val_loss, val_acc, val_mae = evaluate(model_v2_2_1_5, val_loader, device, criterion_age, criterion_gender)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc.item())\n",
    "        val_accuracies.append(val_acc.item())\n",
    "        train_maes.append(train_mae.item())\n",
    "        val_maes.append(val_mae.item())\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Train Loss : {train_loss:.4f}, Train Gender Accuracy : {train_acc:.4f}, Train AGE MAE : {train_mae:.4f}')\n",
    "        print(f'Validation Loss : {val_loss:.4f}, Validation Gender Accuracy : {val_acc:.4f}, Validation AGE MAE : {val_mae:.4f}')\n",
    "\n",
    "        early_stopping(val_loss, model_v2_2_1_5)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        # 평가 안 할 때는 학습 손실만 출력\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] - Training loss: {avg_loss:.4f}')\n",
    "\n",
    "# 가장 좋은 가중치로 복원\n",
    "model_v2_2_1_5.load_state_dict(early_stopping.best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b82925",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "pth_save_path = os.path.join(base_dir, 'pth_pkl', 'model_raw_weights_v2_2_1_5.pth')\n",
    "try:\n",
    "    torch.save(model_v2_2_1_5.state_dict(), pth_save_path)\n",
    "    print(f'모델 저장 완료 → {pth_save_path}')\n",
    "except Exception as e:\n",
    "    print(f'모델 저장 실패: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'train_maes': train_maes,\n",
    "    'val_maes': val_maes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pkl\n",
    "base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "pkl_save_path = os.path.join(base_dir, 'pth_pkl', 'model_raw_v2_2_1_5.pkl')\n",
    "\n",
    "try:\n",
    "    with open(pkl_save_path, \"wb\") as f:\n",
    "        pickle.dump(history, f)\n",
    "    print(f'학습 기록이 성공적으로 저장되었습니다 : {pkl_save_path}')\n",
    "except Exception as e:\n",
    "    print(f'학습 기록 저장 실패: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eunseo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
