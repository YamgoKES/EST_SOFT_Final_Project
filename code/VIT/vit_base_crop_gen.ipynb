{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee898d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc126ff",
   "metadata": {},
   "source": [
    "##### 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = '/workspace/yonghak/data/image_emotion_crop_dataset.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56d884a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터 불러오기\n",
    "data = np.load(SAVE_PATH)\n",
    "images = data['images']     # (전체 개수, 224, 224, 3)\n",
    "emotions = data['emotions'] # (전체 개수,)\n",
    "sets = data['sets']         # (전체 개수,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f5333ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val/test 인덱스 분리\n",
    "train_idx = sets == 'train'\n",
    "val_idx = sets == 'val'\n",
    "test_idx = sets == 'test'\n",
    "\n",
    "train_images = images[train_idx]\n",
    "val_images = images[val_idx]\n",
    "test_images = images[test_idx]\n",
    "train_emotions = emotions[train_idx]\n",
    "val_emotions = emotions[val_idx]\n",
    "test_emotions = emotions[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "591b885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정 클래스: ['기쁨' '당황' '분노' '슬픔']\n"
     ]
    }
   ],
   "source": [
    "# 라벨 인코딩\n",
    "emotion_le = LabelEncoder()\n",
    "train_emotion_idx = emotion_le.fit_transform(train_emotions)\n",
    "val_emotion_idx = emotion_le.transform(val_emotions)\n",
    "test_emotion_idx = emotion_le.transform(test_emotions)\n",
    "NUM_CLASSES = len(emotion_le.classes_)\n",
    "print('감정 클래스:', emotion_le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7e9e003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 이미지 shape: (5994, 224, 224, 3), 감정 라벨 shape: (5994,)\n",
      "Validation 이미지 shape: (1200, 224, 224, 3), 감정 라벨 shape: (1200,)\n",
      "Test 이미지 shape: (1200, 224, 224, 3), 감정 라벨 shape: (1200,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train 이미지 shape: {train_images.shape}, 감정 라벨 shape: {train_emotion_idx.shape}\")\n",
    "print(f\"Validation 이미지 shape: {val_images.shape}, 감정 라벨 shape: {val_emotion_idx.shape}\")\n",
    "print(f\"Test 이미지 shape: {test_images.shape}, 감정 라벨 shape: {test_emotion_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a5baa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6753e08a",
   "metadata": {},
   "source": [
    "##### 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "285a8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "\n",
    "# 학습 데이터 : 데이터 증강 + 전처리\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),   # 좌우반전\n",
    "    transforms.RandomRotation(30),       # -30~+30도 회전\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),  # 밝기, 명암, 색상 변화\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 검증/테스트 : 증강 없이 전처리만\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "test_transform = val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0b64185",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmotionDataset(train_images, train_emotion_idx, transform=train_transform)\n",
    "val_dataset = EmotionDataset(val_images, val_emotion_idx, transform=val_transform)\n",
    "test_dataset = EmotionDataset(test_images, test_emotion_idx, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf5f59",
   "metadata": {},
   "source": [
    "##### 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7008f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT 모델\n",
    "class ViT_EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(ViT_EmotionClassifier, self).__init__()\n",
    "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n",
    "        self.vit.head = nn.Linear(self.vit.num_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViT_EmotionClassifier(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d0931c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001, path='checkpoint.pt'):\n",
    "        self.patience = patience              # 개선 안되는 최대 epoch 수\n",
    "        self.min_delta = min_delta            # 최소 개선 폭\n",
    "        self.counter = 0                      # 개선 안 된 epoch 수\n",
    "        self.best_score = None                # 최고 성능\n",
    "        self.early_stop = False               # 중단 여부\n",
    "        self.path = path                      # 모델 저장 경로\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping 카운트: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f'>>> [Val] 모델 저장됨: {self.path} (Val Acc: {self.best_score:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e841d2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.0747 | Train Acc: 0.5167\n",
      "Validation Loss: 0.8535 | Validation Acc: 0.6183\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop_gen_1.pth (Val Acc: 0.6183)\n",
      "Epoch 2 | Train Loss: 0.7536 | Train Acc: 0.6855\n",
      "Validation Loss: 0.7539 | Validation Acc: 0.6983\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop_gen_1.pth (Val Acc: 0.6983)\n",
      "Epoch 3 | Train Loss: 0.6393 | Train Acc: 0.7376\n",
      "Validation Loss: 0.7327 | Validation Acc: 0.6958\n",
      "EarlyStopping 카운트: 1/3\n",
      "Epoch 4 | Train Loss: 0.5572 | Train Acc: 0.7729\n",
      "Validation Loss: 0.7718 | Validation Acc: 0.7000\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop_gen_1.pth (Val Acc: 0.7000)\n",
      "Epoch 5 | Train Loss: 0.5467 | Train Acc: 0.7828\n",
      "Validation Loss: 0.7872 | Validation Acc: 0.7025\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop_gen_1.pth (Val Acc: 0.7025)\n",
      "Epoch 6 | Train Loss: 0.4742 | Train Acc: 0.8166\n",
      "Validation Loss: 0.8015 | Validation Acc: 0.6983\n",
      "EarlyStopping 카운트: 1/3\n",
      "Epoch 7 | Train Loss: 0.4346 | Train Acc: 0.8308\n",
      "Validation Loss: 0.9013 | Validation Acc: 0.6933\n",
      "EarlyStopping 카운트: 2/3\n",
      "Epoch 8 | Train Loss: 0.3992 | Train Acc: 0.8487\n",
      "Validation Loss: 0.8418 | Validation Acc: 0.7083\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop_gen_1.pth (Val Acc: 0.7083)\n",
      "Epoch 9 | Train Loss: 0.3567 | Train Acc: 0.8610\n",
      "Validation Loss: 0.8522 | Validation Acc: 0.7058\n",
      "EarlyStopping 카운트: 1/3\n",
      "Epoch 10 | Train Loss: 0.3215 | Train Acc: 0.8795\n",
      "Validation Loss: 0.7789 | Validation Acc: 0.7200\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop_gen_1.pth (Val Acc: 0.7200)\n",
      "Epoch 11 | Train Loss: 0.2807 | Train Acc: 0.8954\n",
      "Validation Loss: 0.7732 | Validation Acc: 0.7167\n",
      "EarlyStopping 카운트: 1/3\n",
      "Epoch 12 | Train Loss: 0.2540 | Train Acc: 0.9056\n",
      "Validation Loss: 0.9203 | Validation Acc: 0.7050\n",
      "EarlyStopping 카운트: 2/3\n",
      "Epoch 13 | Train Loss: 0.2389 | Train Acc: 0.9152\n",
      "Validation Loss: 0.9328 | Validation Acc: 0.7033\n",
      "EarlyStopping 카운트: 3/3\n",
      "EarlyStopping: validation 성능이 개선되지 않아 학습을 조기 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "save_path_val = '/workspace/yonghak/vit_base_crop_gen_1.pth'\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001, path=save_path_val)\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    # ---- 학습 ----\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "    train_acc = train_correct / len(train_dataset)\n",
    "    train_loss_avg = train_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss_avg:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # ---- 검증 ----\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "    val_acc = val_correct / len(val_dataset)\n",
    "    val_loss_avg = val_loss / len(val_dataset)\n",
    "    print(f\"Validation Loss: {val_loss_avg:.4f} | Validation Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # ---- Early Stopping 호출 ----\n",
    "    early_stopping(val_acc, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"EarlyStopping: validation 성능이 개선되지 않아 학습을 조기 종료합니다.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55c292da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8528 Test Acc: 0.7167\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "model.eval()\n",
    "test_loss, test_correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_dataset):.4f} \"\n",
    "      f\"Test Acc: {test_correct/len(test_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8441a327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[235  11  20  34]\n",
      " [ 19 208  39  34]\n",
      " [ 20  44 175  61]\n",
      " [  8  12  38 242]]\n",
      "\n",
      " 정확도: 0.7167 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          기쁨       0.83      0.78      0.81       300\n",
      "          당황       0.76      0.69      0.72       300\n",
      "          분노       0.64      0.58      0.61       300\n",
      "          슬픔       0.65      0.81      0.72       300\n",
      "\n",
      "    accuracy                           0.72      1200\n",
      "   macro avg       0.72      0.72      0.72      1200\n",
      "weighted avg       0.72      0.72      0.72      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 혼동행렬\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(cm)\n",
    "acc = np.mean(np.array(all_labels) == np.array(all_preds))\n",
    "print(f\"\\n 정확도: {acc:.4f} \\n\")\n",
    "print(classification_report(all_labels, all_preds, target_names=emotion_le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72481751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7971 | Test Acc: 0.7242\n"
     ]
    }
   ],
   "source": [
    "# 저장된 최고 성능 모델 불러오기\n",
    "model.load_state_dict(torch.load('/workspace/yonghak/vit_base_crop_gen_1.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 테스트 평가\n",
    "test_loss, test_correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_dataset):.4f} | Test Acc: {test_correct/len(test_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69c65224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "혼동행렬:\n",
      " [[244  30  11  15]\n",
      " [ 21 240  27  12]\n",
      " [ 23  81 174  22]\n",
      " [ 13  40  36 211]]\n",
      "\n",
      "정확도: 0.7242\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          기쁨       0.81      0.81      0.81       300\n",
      "          당황       0.61      0.80      0.69       300\n",
      "          분노       0.70      0.58      0.64       300\n",
      "          슬픔       0.81      0.70      0.75       300\n",
      "\n",
      "    accuracy                           0.72      1200\n",
      "   macro avg       0.73      0.72      0.72      1200\n",
      "weighted avg       0.73      0.72      0.72      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 혼동행렬 및 리포트\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동행렬 출력\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"혼동행렬:\\n\", cm)\n",
    "\n",
    "# 정확도\n",
    "acc = np.mean(np.array(all_labels) == np.array(all_preds))\n",
    "print(f\"\\n정확도: {acc:.4f}\\n\")\n",
    "\n",
    "# 클래스별 리포트\n",
    "print(classification_report(all_labels, all_preds, target_names=emotion_le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
