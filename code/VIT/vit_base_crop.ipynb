{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98de2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a152431",
   "metadata": {},
   "source": [
    "##### 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = '/workspace/yonghak/data/image_emotion_crop_dataset.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda20c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터 불러오기\n",
    "data = np.load(SAVE_PATH)\n",
    "images = data['images']     # (전체 개수, 224, 224, 3)\n",
    "emotions = data['emotions'] # (전체 개수,)\n",
    "sets = data['sets']         # (전체 개수,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36ce1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val/test 인덱스 분리\n",
    "train_idx = sets == 'train'\n",
    "val_idx = sets == 'val'\n",
    "test_idx = sets == 'test'\n",
    "\n",
    "train_images = images[train_idx]\n",
    "val_images = images[val_idx]\n",
    "test_images = images[test_idx]\n",
    "train_emotions = emotions[train_idx]\n",
    "val_emotions = emotions[val_idx]\n",
    "test_emotions = emotions[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b97337c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정 클래스: ['기쁨' '당황' '분노' '슬픔']\n"
     ]
    }
   ],
   "source": [
    "# 라벨 인코딩\n",
    "emotion_le = LabelEncoder()\n",
    "train_emotion_idx = emotion_le.fit_transform(train_emotions)\n",
    "val_emotion_idx = emotion_le.transform(val_emotions)\n",
    "test_emotion_idx = emotion_le.transform(test_emotions)\n",
    "NUM_CLASSES = len(emotion_le.classes_)\n",
    "print('감정 클래스:', emotion_le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec3fef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 이미지 shape: (5994, 224, 224, 3), 감정 라벨 shape: (5994,)\n",
      "Validation 이미지 shape: (1200, 224, 224, 3), 감정 라벨 shape: (1200,)\n",
      "Test 이미지 shape: (1200, 224, 224, 3), 감정 라벨 shape: (1200,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train 이미지 shape: {train_images.shape}, 감정 라벨 shape: {train_emotion_idx.shape}\")\n",
    "print(f\"Validation 이미지 shape: {val_images.shape}, 감정 라벨 shape: {val_emotion_idx.shape}\")\n",
    "print(f\"Test 이미지 shape: {test_images.shape}, 감정 라벨 shape: {test_emotion_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70566058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b755fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "common_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90bcd22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmotionDataset(train_images, train_emotion_idx, transform=common_transform)\n",
    "val_dataset = EmotionDataset(val_images, val_emotion_idx, transform=common_transform)\n",
    "test_dataset = EmotionDataset(test_images, test_emotion_idx, transform=common_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34efd47",
   "metadata": {},
   "source": [
    "##### 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT 모델\n",
    "class ViT_EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(ViT_EmotionClassifier, self).__init__()\n",
    "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n",
    "        self.vit.head = nn.Linear(self.vit.num_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViT_EmotionClassifier(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9392eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001, path='checkpoint.pt'):\n",
    "        self.patience = patience              # 개선 안되는 최대 epoch 수\n",
    "        self.min_delta = min_delta            # 최소 개선 폭\n",
    "        self.counter = 0                      # 개선 안 된 epoch 수\n",
    "        self.best_score = None                # 최고 성능\n",
    "        self.early_stop = False               # 중단 여부\n",
    "        self.path = path                      # 모델 저장 경로\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping 카운트: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f'>>> [Val] 모델 저장됨: {self.path} (Val Acc: {self.best_score:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9003c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.4147 | Train Acc: 0.2840\n",
      "Validation Loss: 1.2809 | Validation Acc: 0.3767\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop.pth (Val Acc: 0.3767)\n",
      "Epoch 2 | Train Loss: 0.9488 | Train Acc: 0.5906\n",
      "Validation Loss: 0.9546 | Validation Acc: 0.6000\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop.pth (Val Acc: 0.6000)\n",
      "Epoch 3 | Train Loss: 0.6898 | Train Acc: 0.7191\n",
      "Validation Loss: 0.8758 | Validation Acc: 0.6683\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop.pth (Val Acc: 0.6683)\n",
      "Epoch 4 | Train Loss: 0.5424 | Train Acc: 0.7875\n",
      "Validation Loss: 0.8627 | Validation Acc: 0.6800\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop.pth (Val Acc: 0.6800)\n",
      "Epoch 5 | Train Loss: 0.4373 | Train Acc: 0.8302\n",
      "Validation Loss: 0.8081 | Validation Acc: 0.7025\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop.pth (Val Acc: 0.7025)\n",
      "Epoch 6 | Train Loss: 0.3525 | Train Acc: 0.8629\n",
      "Validation Loss: 0.9705 | Validation Acc: 0.6650\n",
      "EarlyStopping 카운트: 1/3\n",
      "Epoch 7 | Train Loss: 0.3005 | Train Acc: 0.8837\n",
      "Validation Loss: 0.9626 | Validation Acc: 0.6825\n",
      "EarlyStopping 카운트: 2/3\n",
      "Epoch 8 | Train Loss: 0.2348 | Train Acc: 0.9132\n",
      "Validation Loss: 0.9718 | Validation Acc: 0.7125\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base_crop.pth (Val Acc: 0.7125)\n",
      "Epoch 9 | Train Loss: 0.1796 | Train Acc: 0.9349\n",
      "Validation Loss: 1.1364 | Validation Acc: 0.6642\n",
      "EarlyStopping 카운트: 1/3\n",
      "Epoch 10 | Train Loss: 0.1474 | Train Acc: 0.9434\n",
      "Validation Loss: 1.1891 | Validation Acc: 0.6700\n",
      "EarlyStopping 카운트: 2/3\n",
      "Epoch 11 | Train Loss: 0.1191 | Train Acc: 0.9578\n",
      "Validation Loss: 1.3002 | Validation Acc: 0.6858\n",
      "EarlyStopping 카운트: 3/3\n",
      "EarlyStopping: validation 성능이 개선되지 않아 학습을 조기 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "save_path_val = '/workspace/yonghak/vit_base_crop.pth'\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001, path=save_path_val)\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    # ---- 학습 ----\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "    train_acc = train_correct / len(train_dataset)\n",
    "    train_loss_avg = train_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss_avg:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # ---- 검증 ----\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "    val_acc = val_correct / len(val_dataset)\n",
    "    val_loss_avg = val_loss / len(val_dataset)\n",
    "    print(f\"Validation Loss: {val_loss_avg:.4f} | Validation Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # ---- Early Stopping 호출 ----\n",
    "    early_stopping(val_acc, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"EarlyStopping: validation 성능이 개선되지 않아 학습을 조기 종료합니다.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7819e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.2517 Test Acc: 0.6833\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "model.eval()\n",
    "test_loss, test_correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_dataset):.4f} \"\n",
    "      f\"Test Acc: {test_correct/len(test_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "657e36e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[236  29  19  16]\n",
      " [ 20 242  26  12]\n",
      " [ 23  89 159  29]\n",
      " [ 20  50  48 182]]\n",
      "\n",
      " 정확도: 0.6825 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          기쁨       0.79      0.79      0.79       300\n",
      "          당황       0.59      0.81      0.68       300\n",
      "          분노       0.63      0.53      0.58       300\n",
      "          슬픔       0.76      0.61      0.68       300\n",
      "\n",
      "    accuracy                           0.68      1200\n",
      "   macro avg       0.69      0.68      0.68      1200\n",
      "weighted avg       0.69      0.68      0.68      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 혼동행렬\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(cm)\n",
    "acc = np.mean(np.array(all_labels) == np.array(all_preds))\n",
    "print(f\"\\n 정확도: {acc:.4f} \\n\")\n",
    "print(classification_report(all_labels, all_preds, target_names=emotion_le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
