{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65ca2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daaf468",
   "metadata": {},
   "source": [
    "##### **데이터 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d28ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = '/workspace/yonghak/data/image_emotion_dataset.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f48dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "data = np.load(SAVE_PATH)\n",
    "images = data['images']\n",
    "emotions = data['emotions']\n",
    "sets = data['sets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "931380b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val/test 인덱스 분리\n",
    "train_idx = sets == 'train'\n",
    "val_idx = sets == 'val'\n",
    "test_idx = sets == 'test'\n",
    "\n",
    "train_images = images[train_idx]\n",
    "val_images = images[val_idx]\n",
    "test_images = images[test_idx]\n",
    "train_emotions = emotions[train_idx]\n",
    "val_emotions = emotions[val_idx]\n",
    "test_emotions = emotions[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "552cb0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정 클래스: ['기쁨' '당황' '분노' '슬픔']\n"
     ]
    }
   ],
   "source": [
    "# 라벨 인코딩 (감정만)\n",
    "emotion_le = LabelEncoder()\n",
    "train_emotion_idx = emotion_le.fit_transform(train_emotions)\n",
    "val_emotion_idx = emotion_le.transform(val_emotions)\n",
    "test_emotion_idx = emotion_le.transform(test_emotions)\n",
    "NUM_CLASSES = len(emotion_le.classes_)\n",
    "print('감정 클래스:', emotion_le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "581c1d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 이미지 shape: (5994, 224, 224, 3), 감정 라벨 shape: (5994,)\n",
      "Validation 이미지 shape: (1200, 224, 224, 3), 감정 라벨 shape: (1200,)\n",
      "Test 이미지 shape: (1200, 224, 224, 3), 감정 라벨 shape: (1200,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train 이미지 shape: {train_images.shape}, 감정 라벨 shape: {train_emotion_idx.shape}\")\n",
    "print(f\"Validation 이미지 shape: {val_images.shape}, 감정 라벨 shape: {val_emotion_idx.shape}\")\n",
    "print(f\"Test 이미지 shape: {test_images.shape}, 감정 라벨 shape: {test_emotion_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02f80729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스 (감정만 사용)\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f80d400",
   "metadata": {},
   "source": [
    "##### **데이터 증강**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae33a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "\n",
    "# 학습 데이터 : 데이터 증강 + 전처리\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),                     # 좌우반전\n",
    "    transforms.RandomRotation(10),                         # -10~+10도 회전\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),  # 밝기, 명암, 색상 변화\n",
    "    #transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),   # 블러\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.2),     # 일정 영역 무작위 제거\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 검증/테스트 : 증강 없이 전처리만\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "test_transform = val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e23e1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmotionDataset(train_images, train_emotion_idx, transform=train_transform)\n",
    "val_dataset = EmotionDataset(val_images, val_emotion_idx, transform=val_transform)\n",
    "test_dataset = EmotionDataset(test_images, test_emotion_idx, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c5c71",
   "metadata": {},
   "source": [
    "##### **모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79b6bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT 모델 (감정만 분류)\n",
    "class ViT_EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(ViT_EmotionClassifier, self).__init__()\n",
    "        self.vit = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=0)\n",
    "        self.vit.head = nn.Linear(self.vit.num_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViT_EmotionClassifier(num_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e18bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001, path='checkpoint.pt'):\n",
    "        self.patience = patience              # 개선 안되는 최대 epoch 수\n",
    "        self.min_delta = min_delta            # 최소 개선 폭\n",
    "        self.counter = 0                      # 개선 안 된 epoch 수\n",
    "        self.best_score = None                # 최고 성능\n",
    "        self.early_stop = False               # 중단 여부\n",
    "        self.path = path                      # 모델 저장 경로\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping 카운트: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f'>>> [Val] 모델 저장됨: {self.path} (Val Acc: {self.best_score:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22dc144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.1594 | Train Acc: 0.4992\n",
      "Validation Loss: 0.8842 | Validation Acc: 0.6233\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_tiny_gen_erasing.pth (Val Acc: 0.6233)\n",
      "Epoch 2 | Train Loss: 0.8028 | Train Acc: 0.6695\n",
      "Validation Loss: 0.8410 | Validation Acc: 0.6808\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_tiny_gen_erasing.pth (Val Acc: 0.6808)\n",
      "Epoch 3 | Train Loss: 0.6781 | Train Acc: 0.7251\n",
      "Validation Loss: 0.8442 | Validation Acc: 0.6983\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_tiny_gen_erasing.pth (Val Acc: 0.6983)\n",
      "Epoch 4 | Train Loss: 0.6015 | Train Acc: 0.7601\n",
      "Validation Loss: 0.8414 | Validation Acc: 0.6775\n",
      "EarlyStopping 카운트: 1/3\n",
      "Epoch 5 | Train Loss: 0.5457 | Train Acc: 0.7806\n",
      "Validation Loss: 0.8285 | Validation Acc: 0.6842\n",
      "EarlyStopping 카운트: 2/3\n",
      "Epoch 6 | Train Loss: 0.4800 | Train Acc: 0.8071\n",
      "Validation Loss: 0.7476 | Validation Acc: 0.7233\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_tiny_gen_erasing.pth (Val Acc: 0.7233)\n",
      "Epoch 7 | Train Loss: 0.4609 | Train Acc: 0.8213\n",
      "Validation Loss: 0.7984 | Validation Acc: 0.7167\n",
      "EarlyStopping 카운트: 1/3\n",
      "Epoch 8 | Train Loss: 0.3773 | Train Acc: 0.8527\n",
      "Validation Loss: 0.8761 | Validation Acc: 0.7083\n",
      "EarlyStopping 카운트: 2/3\n",
      "Epoch 9 | Train Loss: 0.3577 | Train Acc: 0.8580\n",
      "Validation Loss: 0.9398 | Validation Acc: 0.7250\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_tiny_gen_erasing.pth (Val Acc: 0.7250)\n",
      "Epoch 10 | Train Loss: 0.3193 | Train Acc: 0.8772\n",
      "Validation Loss: 1.0676 | Validation Acc: 0.7075\n",
      "EarlyStopping 카운트: 1/3\n",
      "Epoch 11 | Train Loss: 0.3012 | Train Acc: 0.8745\n",
      "Validation Loss: 0.9778 | Validation Acc: 0.7342\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_tiny_gen_erasing.pth (Val Acc: 0.7342)\n",
      "Epoch 12 | Train Loss: 0.2675 | Train Acc: 0.8967\n",
      "Validation Loss: 1.1346 | Validation Acc: 0.6975\n",
      "EarlyStopping 카운트: 1/3\n",
      "Epoch 13 | Train Loss: 0.2697 | Train Acc: 0.8994\n",
      "Validation Loss: 1.1613 | Validation Acc: 0.6733\n",
      "EarlyStopping 카운트: 2/3\n",
      "Epoch 14 | Train Loss: 0.2518 | Train Acc: 0.9049\n",
      "Validation Loss: 1.0093 | Validation Acc: 0.7083\n",
      "EarlyStopping 카운트: 3/3\n",
      "EarlyStopping: validation 성능이 개선되지 않아 학습을 조기 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "save_path_val = '/workspace/yonghak/vit_tiny_gen_erasing.pth'\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001, path=save_path_val)\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    # ---- 학습 ----\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "    train_acc = train_correct / len(train_dataset)\n",
    "    train_loss_avg = train_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss_avg:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # ---- 검증 ----\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "    val_acc = val_correct / len(val_dataset)\n",
    "    val_loss_avg = val_loss / len(val_dataset)\n",
    "    print(f\"Validation Loss: {val_loss_avg:.4f} | Validation Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # ---- Early Stopping 호출 ----\n",
    "    early_stopping(val_acc, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"EarlyStopping: validation 성능이 개선되지 않아 학습을 조기 종료합니다.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "765f2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0389 Test Acc: 0.7192\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "model.eval()\n",
    "test_loss, test_correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "print(f\"Test Loss: {test_loss/len(test_dataset):.4f} \"\n",
    "      f\"Test Acc: {test_correct/len(test_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "084cc2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[253   5  30  12]\n",
      " [ 14 239  39   8]\n",
      " [ 23  52 195  30]\n",
      " [ 23  30  71 176]]\n",
      "\n",
      " 정확도: 0.7192 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          기쁨       0.81      0.84      0.83       300\n",
      "          당황       0.73      0.80      0.76       300\n",
      "          분노       0.58      0.65      0.61       300\n",
      "          슬픔       0.78      0.59      0.67       300\n",
      "\n",
      "    accuracy                           0.72      1200\n",
      "   macro avg       0.73      0.72      0.72      1200\n",
      "weighted avg       0.73      0.72      0.72      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동행렬\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(cm)\n",
    "\n",
    "# 정확도(accuracy)\n",
    "acc = np.mean(np.array(all_labels) == np.array(all_preds))\n",
    "print(f\"\\n 정확도: {acc:.4f} \\n\")\n",
    "\n",
    "# precision/recall/f1-score\n",
    "print(classification_report(all_labels, all_preds, target_names=emotion_le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
