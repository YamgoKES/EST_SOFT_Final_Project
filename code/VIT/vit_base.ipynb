{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0368ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a293bc",
   "metadata": {},
   "source": [
    "##### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = '/workspace/yonghak/data/image_emotion_dataset.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f08fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "data = np.load(SAVE_PATH)\n",
    "images = data['images']\n",
    "emotions = data['emotions']\n",
    "sets = data['sets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41b65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val/test 인덱스 분리\n",
    "train_idx = sets == 'train'\n",
    "val_idx = sets == 'val'\n",
    "test_idx = sets == 'test'\n",
    "\n",
    "train_images = images[train_idx]\n",
    "val_images = images[val_idx]\n",
    "test_images = images[test_idx]\n",
    "train_emotions = emotions[train_idx]\n",
    "val_emotions = emotions[val_idx]\n",
    "test_emotions = emotions[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4956298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정 클래스: ['기쁨' '당황' '분노' '슬픔']\n"
     ]
    }
   ],
   "source": [
    "# 라벨 인코딩 (감정만)\n",
    "emotion_le = LabelEncoder()\n",
    "train_emotion_idx = emotion_le.fit_transform(train_emotions)\n",
    "val_emotion_idx = emotion_le.transform(val_emotions)\n",
    "test_emotion_idx = emotion_le.transform(test_emotions)\n",
    "NUM_CLASSES = len(emotion_le.classes_)\n",
    "\n",
    "print('감정 클래스:', emotion_le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14df335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 이미지 shape: (5994, 224, 224, 3), 감정 라벨 shape: (5994,)\n",
      "Validation 이미지 shape: (1200, 224, 224, 3), 감정 라벨 shape: (1200,)\n",
      "Test 이미지 shape: (1200, 224, 224, 3), 감정 라벨 shape: (1200,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train 이미지 shape: {train_images.shape}, 감정 라벨 shape: {train_emotion_idx.shape}\")\n",
    "print(f\"Validation 이미지 shape: {val_images.shape}, 감정 라벨 shape: {val_emotion_idx.shape}\")\n",
    "print(f\"Test 이미지 shape: {test_images.shape}, 감정 라벨 shape: {test_emotion_idx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d39c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스 (감정만 사용)\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.fromarray(image.astype('uint8'))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a947f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "common_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f14d944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmotionDataset(train_images, train_emotion_idx, transform=common_transform)\n",
    "val_dataset = EmotionDataset(val_images, val_emotion_idx, transform=common_transform)\n",
    "test_dataset = EmotionDataset(test_images, test_emotion_idx, transform=common_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdfafdc",
   "metadata": {},
   "source": [
    "##### 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f9b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT 모델 (감정만 분류)\n",
    "class ViT_EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, dropout_rate=0.3):\n",
    "        super(ViT_EmotionClassifier, self).__init__()\n",
    "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "        in_features = self.vit.head.in_features\n",
    "        self.vit.head = nn.Identity()  # 기존 head 제거\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc = nn.Linear(in_features, num_classes)  # 새 head 정의\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViT_EmotionClassifier(num_classes=NUM_CLASSES, dropout_rate=0.3)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95ea7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001, path='checkpoint.pt'):\n",
    "        self.patience = patience              # 개선 안되는 최대 epoch 수\n",
    "        self.min_delta = min_delta            # 최소 개선 폭\n",
    "        self.counter = 0                      # 개선 안 된 epoch 수\n",
    "        self.best_score = None                # 최고 성능\n",
    "        self.early_stop = False               # 중단 여부\n",
    "        self.path = path                      # 모델 저장 경로\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping 카운트: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f'>>> [Val] 모델 저장됨: {self.path} (Val Acc: {self.best_score:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e708432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.4469 | Train Acc: 0.2504\n",
      "Validation Loss: 1.4272 | Validation Acc: 0.2500\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base.pth (Val Acc: 0.2500)\n",
      "Epoch 2 | Train Loss: 1.2479 | Train Acc: 0.4169\n",
      "Validation Loss: 1.1491 | Validation Acc: 0.5267\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base.pth (Val Acc: 0.5267)\n",
      "Epoch 3 | Train Loss: 0.8200 | Train Acc: 0.6705\n",
      "Validation Loss: 0.9132 | Validation Acc: 0.6108\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base.pth (Val Acc: 0.6108)\n",
      "Epoch 4 | Train Loss: 0.4985 | Train Acc: 0.8061\n",
      "Validation Loss: 0.9654 | Validation Acc: 0.6575\n",
      ">>> [Val] 모델 저장됨: /workspace/yonghak/vit_base.pth (Val Acc: 0.6575)\n",
      "Epoch 5 | Train Loss: 0.3079 | Train Acc: 0.8859\n",
      "Validation Loss: 1.2324 | Validation Acc: 0.6442\n",
      "EarlyStopping 카운트: 1/3\n",
      "Epoch 6 | Train Loss: 0.2076 | Train Acc: 0.9236\n",
      "Validation Loss: 1.6125 | Validation Acc: 0.6500\n",
      "EarlyStopping 카운트: 2/3\n",
      "Epoch 7 | Train Loss: 0.1392 | Train Acc: 0.9483\n",
      "Validation Loss: 1.4238 | Validation Acc: 0.6358\n",
      "EarlyStopping 카운트: 3/3\n",
      "EarlyStopping: validation 성능이 개선되지 않아 학습을 조기 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "save_path_val = '/workspace/yonghak/vit_base.pth'\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001, path=save_path_val)\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    # ---- 학습 ----\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "    train_acc = train_correct / len(train_dataset)\n",
    "    train_loss_avg = train_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss_avg:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # ---- 검증 ----\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "    val_acc = val_correct / len(val_dataset)\n",
    "    val_loss_avg = val_loss / len(val_dataset)\n",
    "    print(f\"Validation Loss: {val_loss_avg:.4f} | Validation Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # ---- Early Stopping 호출 ----\n",
    "    early_stopping(val_acc, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"EarlyStopping: validation 성능이 개선되지 않아 학습을 조기 종료합니다.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d1f5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.2912 Test Acc: 0.6533\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "model.eval()\n",
    "test_loss, test_correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "print(f\"Test Loss: {test_loss/len(test_dataset):.4f} \"\n",
    "      f\"Test Acc: {test_correct/len(test_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "176dc4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[221  14  42  23]\n",
      " [ 10 199  76  15]\n",
      " [ 10  49 194  47]\n",
      " [ 25  27  78 170]]\n",
      "\n",
      " 정확도: 0.6533 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          기쁨       0.83      0.74      0.78       300\n",
      "          당황       0.69      0.66      0.68       300\n",
      "          분노       0.50      0.65      0.56       300\n",
      "          슬픔       0.67      0.57      0.61       300\n",
      "\n",
      "    accuracy                           0.65      1200\n",
      "   macro avg       0.67      0.65      0.66      1200\n",
      "weighted avg       0.67      0.65      0.66      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 혼동행렬\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(cm)\n",
    "\n",
    "# 정확도(accuracy)\n",
    "acc = np.mean(np.array(all_labels) == np.array(all_preds))\n",
    "print(f\"\\n 정확도: {acc:.4f} \\n\")\n",
    "\n",
    "# precision/recall/f1-score\n",
    "print(classification_report(all_labels, all_preds, target_names=emotion_le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
