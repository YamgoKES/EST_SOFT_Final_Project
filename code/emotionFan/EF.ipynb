{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea8fce2-8ca7-4489-99c2-cf9871c2639b",
   "metadata": {},
   "source": [
    "# 코드셀 0 - Emotion-FAN GitHub에서 코드 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01d2268-4fc6-4f24-9141-fe28a2383823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Emotion-FAN' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Open-Debin/Emotion-FAN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072cd81e-5801-4ef2-8bfc-7b38857119dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.160-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in g:\\anaconda3\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in g:\\anaconda3\\lib\\site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\sck\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\sck\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in g:\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in g:\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in g:\\anaconda3\\lib\\site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\sck\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\sck\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics) (0.17.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in g:\\anaconda3\\lib\\site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in g:\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in g:\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in g:\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in g:\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in g:\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in g:\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in g:\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in g:\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in g:\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in g:\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in g:\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in g:\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in g:\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in g:\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in g:\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sck\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.8.0->ultralytics) (4.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sck\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in g:\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in g:\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in g:\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: colorama in g:\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in g:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in g:\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in g:\\anaconda3\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading ultralytics-8.3.160-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 25.1 MB/s eta 0:00:00\n",
      "Downloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: ultralytics-thop, ultralytics\n",
      "Successfully installed ultralytics-8.3.160 ultralytics-thop-2.0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts ultralytics.exe and yolo.exe are installed in 'C:\\Users\\SCK\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71feec3b-05e8-4e20-b26f-7e6fdb2e252d",
   "metadata": {},
   "source": [
    "# 코드셀 1 Custom Dataset 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7826a3d7-c265-4579-8321-11501aae7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# YOLOv8 얼굴 탐지 모델 로딩 (한 번만 실행)\n",
    "yolo_model = YOLO(\"yolov8n-face.pt\")\n",
    "\n",
    "class CustomEmotionFolder(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, face_detector=None):\n",
    "        valid_exts = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "        def has_valid_extension(path):\n",
    "            return path.lower().endswith(valid_exts)\n",
    "\n",
    "        super().__init__(root, transform=transform, is_valid_file=has_valid_extension)\n",
    "        self.class_mapping = self.class_to_idx\n",
    "        self.face_detector = face_detector\n",
    "        print(\"✅ class_to_idx:\", self.class_mapping)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(path).convert('RGB')\n",
    "            image = ImageOps.exif_transpose(image)\n",
    "\n",
    "            if self.face_detector:\n",
    "                img_np = np.array(image)\n",
    "                results = self.face_detector(img_np, verbose=False)[0]\n",
    "\n",
    "                if results.boxes is not None and len(results.boxes) > 0:\n",
    "                    boxes = results.boxes.xyxy.cpu().numpy()\n",
    "                    largest = max(boxes, key=lambda b: (b[2] - b[0]) * (b[3] - b[1]))\n",
    "                    x1, y1, x2, y2 = map(int, largest)\n",
    "\n",
    "                    W, H = image.size\n",
    "                    x1, x2 = sorted((max(0, x1), min(W, x2)))\n",
    "                    y1, y2 = sorted((max(0, y1), min(H, y2)))\n",
    "                    image = image.crop((x1, y1, x2, y2))\n",
    "                else:\n",
    "                    return None  # 얼굴 검출 실패 → 스킵\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 이미지 로딩 실패: {path} / {e}\")\n",
    "            return None\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "def collate_fn_remove_none(batch):\n",
    "    # None이 아닌 샘플만 모아서 튜플로 반환\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), torch.empty(0)\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df73cc7-e443-4c7a-8c0c-3887290a3b9f",
   "metadata": {},
   "source": [
    "# 코드셀 2 ResNet의 기본 블록 정의 (BasicBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13195807-4dba-4a10-85de-17cfc2a1819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a569bf9-5ced-4dd7-a6c3-9b35339418e8",
   "metadata": {},
   "source": [
    "# 코드셀 3  Emotion-FAN의 전체 모델 구조 (ResNet + Attention) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34f45a0-48e0-4cce-b8b5-4de3c8e7614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNet_AT_Attention(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=4, at_type='self-attention', dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.inplanes = 64\n",
    "        self.at_type = at_type\n",
    "\n",
    "        # 기본 ResNet18 구조\n",
    "        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(3, 2, 1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], 2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], 2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], 2)\n",
    "\n",
    "        # Dropout layers 추가 (각 block 뒤에 삽입)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)  # layer2 뒤\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)  # layer3 뒤\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)  # layer4 뒤\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # 기존 FC 앞 dropout 유지\n",
    "\n",
    "        # attention 계산용 α\n",
    "        self.alpha = nn.Sequential(\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # 감정 예측용 FC\n",
    "        self.pred_fc1 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x=None, phrase=\"eval\", AT_level=\"first_level\", vm=None):\n",
    "        if phrase == \"eval\" and AT_level == \"first_level\":\n",
    "            x = self.relu(self.bn1(self.conv1(x)))\n",
    "            x = self.maxpool(x)\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.dropout1(x)  # ⬅ dropout1 추가\n",
    "            x = self.layer3(x)\n",
    "            x = self.dropout2(x)  # ⬅ dropout2 추가\n",
    "            x = self.layer4(x)\n",
    "            x = self.dropout3(x)  # ⬅ dropout3 추가\n",
    "            x = self.avgpool(x).squeeze(-1).squeeze(-1)  # shape: (B, 512)\n",
    "            alpha = self.alpha(self.dropout(x))  # shape: (B, 1)\n",
    "            return x, alpha\n",
    "\n",
    "        elif phrase == \"eval\" and AT_level == \"pred\":\n",
    "            return self.pred_fc1(self.dropout(vm))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid phrase or AT_level\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a1d8e-e344-49ce-af6e-79f6db5bd74f",
   "metadata": {},
   "source": [
    "# 코드셀 3-1   입력 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d98a2f0-28d5-4444-8fdf-6cd9e6baf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ✅ 정규화 계산 함수\n",
    "def compute_crop_mean_std(image_root, face_detector, sample_size=5000, resize=(224, 224)):\n",
    "    transform_temp = transforms.Compose([\n",
    "        transforms.Resize(resize),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    valid_exts = ('.jpg', '.jpeg', '.png')\n",
    "    image_paths = []\n",
    "\n",
    "    for class_folder in os.listdir(image_root):\n",
    "        class_path = os.path.join(image_root, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        for fname in os.listdir(class_path):\n",
    "            if fname.lower().endswith(valid_exts):\n",
    "                image_paths.append(os.path.join(class_path, fname))\n",
    "\n",
    "    sampled_paths = random.sample(image_paths, min(sample_size, len(image_paths)))\n",
    "\n",
    "    img_mean = torch.zeros(3)\n",
    "    img_std = torch.zeros(3)\n",
    "    valid_count = 0\n",
    "\n",
    "    print(f\"🔍 총 샘플 수: {len(sampled_paths)}\")\n",
    "\n",
    "    for i, path in enumerate(sampled_paths, 1):\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img = ImageOps.exif_transpose(img)\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        results = face_detector(img_np, verbose=False)[0]\n",
    "        if results.boxes is not None and len(results.boxes) > 0:\n",
    "            boxes = results.boxes.xyxy.cpu().numpy()\n",
    "            x1, y1, x2, y2 = map(int, max(boxes, key=lambda b: (b[2]-b[0]) * (b[3]-b[1])))\n",
    "\n",
    "            W, H = img.size\n",
    "            x1, x2 = sorted((max(0, x1), min(W, x2)))\n",
    "            y1, y2 = sorted((max(0, y1), min(H, y2)))\n",
    "\n",
    "            face_pil = img.crop((x1, y1, x2, y2))\n",
    "            tensor = transform_temp(face_pil)\n",
    "\n",
    "            img_mean += tensor.mean(dim=(1, 2))\n",
    "            img_std += tensor.std(dim=(1, 2))\n",
    "            valid_count += 1\n",
    "\n",
    "        # 진행률 출력\n",
    "        print(f\"\\r✅ 진행률: {i}/{len(sampled_paths)} | 검출 성공: {valid_count}\", end='')\n",
    "\n",
    "    print()  # 줄 바꿈\n",
    "\n",
    "    if valid_count == 0:\n",
    "        raise ValueError(\"❌ 얼굴 검출에 성공한 이미지가 없습니다.\")\n",
    "\n",
    "    img_mean /= valid_count\n",
    "    img_std /= valid_count\n",
    "\n",
    "    print(\"📊 얼굴 crop 기준 정규화 정보\")\n",
    "    print(\"Mean:\", img_mean)\n",
    "    print(\"Std: \", img_std)\n",
    "\n",
    "    return img_mean, img_std\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f4323d3-d094-4c16-9de9-11499b697b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 총 샘플 수: 5000\n",
      "✅ 진행률: 5000/5000 | 검출 성공: 4923\n",
      "📊 얼굴 crop 기준 정규화 정보\n",
      "Mean: tensor([0.6055, 0.4580, 0.3965])\n",
      "Std:  tensor([0.2196, 0.1959, 0.1853])\n"
     ]
    }
   ],
   "source": [
    "image_root = r\"C:\\Users\\SCK\\Desktop\\affectnet_split\\train\"\n",
    "#image_root = r\"C:\\Users\\SCK\\Desktop\\Data\\img\\train\"\n",
    "img_mean, img_std = compute_crop_mean_std(image_root, yolo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f0d15-07fb-4723-b212-81bb8ed52dbe",
   "metadata": {},
   "source": [
    "# 코드셀 4 데이터 준비 + Optimizer, Loss, Scheduler 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "545bd27b-f911-4073-abcc-846af62c4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "# ✅ 클래스별 가중치 (튜닝이 없을 때 기본값)\n",
    "class_weights = [1.5, 1.0, 1.0, 1.3]  # anger, happy, panic, sadness\n",
    "\n",
    "# ✅ Focal Loss 정의\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = F.log_softmax(input, dim=1)\n",
    "        p = torch.exp(logp)\n",
    "        logp = (1 - p) ** self.gamma * logp\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha = torch.tensor(self.alpha).to(input.device)\n",
    "            logp = alpha[target] * logp.gather(1, target.unsqueeze(1))\n",
    "\n",
    "        loss = -logp.gather(1, target.unsqueeze(1)).squeeze()\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
    "\n",
    "# ✅ 데이터셋 로딩 함수\n",
    "def get_data_loaders(train_dir, val_dir, img_mean, img_std, face_detector, batch_size):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=img_mean.tolist(), std=img_std.tolist())\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=img_mean.tolist(), std=img_std.tolist())\n",
    "    ])\n",
    "\n",
    "    train_dataset = CustomEmotionFolder(train_dir, transform=train_transform, face_detector=face_detector)\n",
    "    val_dataset   = CustomEmotionFolder(val_dir, transform=val_transform, face_detector=face_detector)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn_remove_none  # ✅ 이거 추가\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn_remove_none  # ✅ 이거 추가\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# ✅ 학습 구성 함수 (Optuna 결과 반영 가능하도록 수정됨)\n",
    "def get_training_components(model,\n",
    "                            lr=1e-3,\n",
    "                            weight_decay=1e-4,\n",
    "                            label_smooth=0.1,\n",
    "                            use_focal=True,\n",
    "                            gamma=2.0,\n",
    "                            alpha=None):\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    if use_focal:\n",
    "        # alpha가 None이거나 list가 아니면 기본값 사용\n",
    "        if alpha is None or not isinstance(alpha, (list, tuple)):\n",
    "            alpha = class_weights\n",
    "        criterion = FocalLoss(gamma=gamma, alpha=alpha)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=label_smooth)\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=2\n",
    "    )\n",
    "\n",
    "    return optimizer, criterion, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c0a41-dced-4af4-afe0-176dd42eac1e",
   "metadata": {},
   "source": [
    "# 코드셀 5 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1928c544-7cbe-4576-911f-2875be3e011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# ✅ 사전학습 가중치 경로\n",
    "weight_path = r\"C:\\Users\\SCK\\Emotion_FAN\\pretrain_model\\Resnet18_FER+_pytorch.pth.tar\"\n",
    "\n",
    "# ✅ 사전학습 백본 로드 함수\n",
    "def load_pretrained_backbone(model, weight_path):\n",
    "    ckpt = torch.load(weight_path, map_location=\"cpu\", weights_only=False)\n",
    "    state_dict = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
    "    cleaned = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    filtered = {k: v for k, v in cleaned.items() if k in model.state_dict() and 'pred_fc1' not in k and 'alpha' not in k}\n",
    "    model.load_state_dict(filtered, strict=False)\n",
    "    print(\"✅ 사전학습 가중치 로드 완료 (fc 제외)\")\n",
    "\n",
    "# ✅ Optuna 목적 함수\n",
    "def objective(trial: Trial, face_detector, img_mean, img_std):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 🔧 하이퍼파라미터 탐색\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout', 0.3, 0.7)\n",
    "    gamma = trial.suggest_float('gamma', 1.0, 5.0)\n",
    "    label_smooth = trial.suggest_float(\"label_smoothing\", 0.0, 0.2)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"adamw\", \"sgd\"])\n",
    "    use_focal = trial.suggest_categorical(\"use_focal\", [True, False])\n",
    "\n",
    "    # ✅ 클래스별 alpha 튜닝 (4개 클래스: anger, happy, panic, sadness)\n",
    "    alpha = [\n",
    "        trial.suggest_float('alpha_anger',   0.5, 2.0),\n",
    "        trial.suggest_float('alpha_happy',   0.5, 2.0),\n",
    "        trial.suggest_float('alpha_panic',   0.5, 2.0),\n",
    "        trial.suggest_float('alpha_sadness', 0.5, 2.0)\n",
    "    ]\n",
    "\n",
    "    # ✅ 모델 구성 및 백본 로딩\n",
    "    model = ResNet_AT_Attention(BasicBlock, [2, 2, 2, 2], num_classes=4, dropout_rate=dropout_rate).to(device)\n",
    "    load_pretrained_backbone(model, weight_path)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for name, module in model.named_children():\n",
    "        if name in ['layer2', 'layer3', 'layer4', 'pred_fc1', 'alpha']:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # ✅ 훈련용 Subset 구성\n",
    "    train_dir = r\"C:\\Users\\SCK\\Desktop\\affectnet_split\\train\"\n",
    "    batch_size = 32\n",
    "    subset_size = 500\n",
    "\n",
    "    full_dataset = CustomEmotionFolder(train_dir, transform=transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=img_mean.tolist(), std=img_std.tolist())\n",
    "    ]), face_detector=face_detector)\n",
    "\n",
    "    labels = [full_dataset[i][1] for i in range(len(full_dataset))]\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=subset_size / len(full_dataset), random_state=42)\n",
    "    indices, _ = next(sss.split(np.zeros(len(labels)), labels))\n",
    "    subset = Subset(full_dataset, indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # ✅ 옵티마이저 구성\n",
    "    params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'adamw':\n",
    "        optimizer = torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(params, lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
    "\n",
    "    # ✅ 손실 함수 구성\n",
    "    if use_focal:\n",
    "        criterion = FocalLoss(gamma=gamma, alpha=alpha)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss(label_smoothing=label_smooth)\n",
    "\n",
    "    # ✅ 학습 및 평가 (간단하게 2 epoch)\n",
    "    model.train()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for epoch in range(1):\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Tuning Epoch {epoch+1}\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features, _ = model(images, phrase=\"eval\", AT_level=\"first_level\")\n",
    "            outputs = model(None, phrase=\"eval\", AT_level=\"pred\", vm=features)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = outputs.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27041c98-4050-470c-bf2c-e77a29bcf17e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 09:44:53,086] A new study created in memory with name: no-name-5d7b04bb-93de-43f2-9794-3732cb6c0dd6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 10:01:25,228] Trial 0 finished with value: 0.8456349645625636 and parameters: {'lr': 0.0002547878584517958, 'weight_decay': 1.1019323329282125e-06, 'dropout': 0.49274433239122695, 'gamma': 2.474464374623031, 'alpha': 0.3340506815773031, 'label_smoothing': 0.019861874243776237, 'optimizer': 'adamw', 'use_focal': True}. Best is trial 0 with value: 0.8456349645625636.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 10:17:35,632] Trial 1 finished with value: 0.8532772262606967 and parameters: {'lr': 0.0005599628108249521, 'weight_decay': 0.0017184645830374244, 'dropout': 0.37647491631235813, 'gamma': 3.047649275789956, 'alpha': 0.9361859197427009, 'label_smoothing': 0.12817079144991153, 'optimizer': 'adamw', 'use_focal': True}. Best is trial 1 with value: 0.8532772262606967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 10:33:51,645] Trial 2 finished with value: 0.71858316120959 and parameters: {'lr': 0.000807651755184874, 'weight_decay': 1.1383830931948762e-05, 'dropout': 0.37839907883417967, 'gamma': 2.2971112732242123, 'alpha': 0.25068925642272377, 'label_smoothing': 0.0974923660322913, 'optimizer': 'sgd', 'use_focal': False}. Best is trial 1 with value: 0.8532772262606967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 10:50:08,899] Trial 3 finished with value: 0.7939206741978766 and parameters: {'lr': 0.002134308375011946, 'weight_decay': 0.0001303894930593582, 'dropout': 0.44018793673199524, 'gamma': 2.1409704237645437, 'alpha': 0.1617727998473359, 'label_smoothing': 0.10560660868056693, 'optimizer': 'adamw', 'use_focal': True}. Best is trial 1 with value: 0.8532772262606967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 11:06:28,377] Trial 4 finished with value: 0.8207597481508613 and parameters: {'lr': 0.000836983023554634, 'weight_decay': 3.410613655023219e-05, 'dropout': 0.6030956804897667, 'gamma': 1.8965268005000606, 'alpha': 0.5441195391455738, 'label_smoothing': 0.029667239106397195, 'optimizer': 'adamw', 'use_focal': True}. Best is trial 1 with value: 0.8532772262606967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 11:22:41,782] Trial 5 finished with value: 0.7710152830137913 and parameters: {'lr': 0.005243630780704211, 'weight_decay': 2.003709866016539e-05, 'dropout': 0.5239887479333623, 'gamma': 4.134756472760406, 'alpha': 0.9708777596982491, 'label_smoothing': 0.03624248115400761, 'optimizer': 'adamw', 'use_focal': False}. Best is trial 1 with value: 0.8532772262606967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 11:39:08,890] Trial 6 finished with value: 0.8507137633474894 and parameters: {'lr': 0.0001223856553291706, 'weight_decay': 0.00043946166961049643, 'dropout': 0.3843371774089505, 'gamma': 2.7826822413623566, 'alpha': 0.1754467111626877, 'label_smoothing': 0.11834514484568265, 'optimizer': 'adamw', 'use_focal': True}. Best is trial 1 with value: 0.8532772262606967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 11:55:34,103] Trial 7 finished with value: 0.5873330852433358 and parameters: {'lr': 0.000708885789173877, 'weight_decay': 0.00011128692643474217, 'dropout': 0.6012349903500718, 'gamma': 1.0325445684782553, 'alpha': 0.6806435808231972, 'label_smoothing': 0.085101672850134, 'optimizer': 'sgd', 'use_focal': True}. Best is trial 1 with value: 0.8532772262606967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 12:10:46,377] Trial 8 finished with value: 0.8235229881737676 and parameters: {'lr': 0.0043151241742211, 'weight_decay': 0.00010329678602734875, 'dropout': 0.31583401222345026, 'gamma': 1.1656398045367382, 'alpha': 0.7239651572194791, 'label_smoothing': 0.04482240027195901, 'optimizer': 'sgd', 'use_focal': True}. Best is trial 1 with value: 0.8532772262606967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 12:25:40,685] Trial 9 finished with value: 0.40646646909416095 and parameters: {'lr': 0.00010765358823013963, 'weight_decay': 0.0064132726587263385, 'dropout': 0.31085646773551856, 'gamma': 3.6659242613623095, 'alpha': 0.5220751978497832, 'label_smoothing': 0.11380804429989477, 'optimizer': 'sgd', 'use_focal': True}. Best is trial 1 with value: 0.8532772262606967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 12:40:40,789] Trial 10 finished with value: 0.42441178417629 and parameters: {'lr': 1.0157389852343734e-05, 'weight_decay': 0.008380793829544878, 'dropout': 0.6547255308689409, 'gamma': 4.989283111967287, 'alpha': 0.8709401841888074, 'label_smoothing': 0.18694765978476605, 'optimizer': 'adam', 'use_focal': False}. Best is trial 1 with value: 0.8532772262606967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 12:55:39,613] Trial 11 finished with value: 0.790110695778403 and parameters: {'lr': 5.111267349846751e-05, 'weight_decay': 0.0011304987635155346, 'dropout': 0.3955339736500943, 'gamma': 3.3574008395434123, 'alpha': 0.3754937591698184, 'label_smoothing': 0.15052181403116893, 'optimizer': 'adamw', 'use_focal': True}. Best is trial 1 with value: 0.8532772262606967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-06-30 13:05:05,697] Trial 12 failed with parameters: {'lr': 4.031370286578268e-05, 'weight_decay': 0.0007253513448048919, 'dropout': 0.39104248463533764, 'gamma': 3.027774636640373, 'alpha': 0.7833953625858975, 'label_smoothing': 0.14369746030472666, 'optimizer': 'adam', 'use_focal': True} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SCK\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SCK\\AppData\\Local\\Temp\\ipykernel_6024\\3875402055.py\", line 116, in objective\n",
      "    features, _ = model(images, phrase=\"eval\", AT_level=\"first_level\")\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SCK\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SCK\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SCK\\AppData\\Local\\Temp\\ipykernel_6024\\832559781.py\", line 53, in forward\n",
      "    x = self.relu(self.bn1(self.conv1(x)))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SCK\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SCK\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SCK\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\batchnorm.py\", line 193, in forward\n",
      "    return F.batch_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SCK\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py\", line 2822, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-06-30 13:05:05,709] Trial 12 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m      2\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_startup_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 5\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎯 Best params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Best Macro F1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_value)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     _optimize(\n\u001b[0;32m    490\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    491\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    492\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    493\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    494\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    495\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    496\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    497\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    498\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    499\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m         _optimize_sequential(\n\u001b[0;32m     65\u001b[0m             study,\n\u001b[0;32m     66\u001b[0m             func,\n\u001b[0;32m     67\u001b[0m             n_trials,\n\u001b[0;32m     68\u001b[0m             timeout,\n\u001b[0;32m     69\u001b[0m             catch,\n\u001b[0;32m     70\u001b[0m             callbacks,\n\u001b[0;32m     71\u001b[0m             gc_after_trial,\n\u001b[0;32m     72\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     74\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     75\u001b[0m         )\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    252\u001b[0m ):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[13], line 116\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    114\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    115\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 116\u001b[0m features, _ \u001b[38;5;241m=\u001b[39m model(images, phrase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m, AT_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_level\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28;01mNone\u001b[39;00m, phrase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m, AT_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m, vm\u001b[38;5;241m=\u001b[39mfeatures)\n\u001b[0;32m    118\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 53\u001b[0m, in \u001b[0;36mResNet_AT_Attention.forward\u001b[1;34m(self, x, phrase, AT_level, vm)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, phrase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m, AT_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_level\u001b[39m\u001b[38;5;124m\"\u001b[39m, vm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phrase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m AT_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_level\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 53\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[0;32m     54\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m     55\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    202\u001b[0m     bn_training,\n\u001b[0;32m    203\u001b[0m     exponential_average_factor,\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    205\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py:2822\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2820\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m   2823\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2824\u001b[0m     weight,\n\u001b[0;32m   2825\u001b[0m     bias,\n\u001b[0;32m   2826\u001b[0m     running_mean,\n\u001b[0;32m   2827\u001b[0m     running_var,\n\u001b[0;32m   2828\u001b[0m     training,\n\u001b[0;32m   2829\u001b[0m     momentum,\n\u001b[0;32m   2830\u001b[0m     eps,\n\u001b[0;32m   2831\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled,\n\u001b[0;32m   2832\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ✅ Optuna 스터디 생성 및 실행\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n",
    ")\n",
    "study.optimize(lambda trial: objective(trial, face_detector, img_mean, img_std), n_trials=100)\n",
    "\n",
    "# ✅ 결과 출력\n",
    "print(\"🎯 Best params:\", study.best_params)\n",
    "print(\"📊 Best Macro F1:\", study.best_value)\n",
    "\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0ed91-2aed-4b87-9a89-876d55f090f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# best_params만 저장\n",
    "with open(\"best_hyperparams.json\", \"w\") as f:\n",
    "    json.dump(study.best_params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c95d6e3a-febf-4fe7-bae5-aad302bbf039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 불러온 하이퍼파라미터:\n",
      "{'lr': 0.00035535733759076474, 'weight_decay': 0.00012145648201289749, 'dropout': 0.30046245637123054, 'gamma': 1.474881640847121, 'label_smoothing': 0.02511914616985348, 'optimizer': 'adamw', 'alpha_anger': 0.7001358928255927, 'alpha_happy': 1.145787238039491, 'alpha_panic': 1.0260425562585442, 'alpha_sadness': 0.9407545081321703}\n"
     ]
    }
   ],
   "source": [
    "# ✅ 사전학습 가중치 경로\n",
    "weight_path = r\"C:\\Users\\SCK\\Emotion_FAN\\pretrain_model\\Resnet18_FER+_pytorch.pth.tar\"\n",
    "\n",
    "# ✅ 사전학습 백본 로드 함수\n",
    "def load_pretrained_backbone(model, weight_path):\n",
    "    ckpt = torch.load(weight_path, map_location=\"cpu\", weights_only=False)\n",
    "    state_dict = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n",
    "    cleaned = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    filtered = {k: v for k, v in cleaned.items() if k in model.state_dict() and 'pred_fc1' not in k and 'alpha' not in k}\n",
    "    model.load_state_dict(filtered, strict=False)\n",
    "    print(\"✅ 사전학습 가중치 로드 완료 (fc 제외)\")\n",
    "\n",
    "# 저장된 JSON 파일 경로\n",
    "load_path = \"best_hyperparams.json\"\n",
    "\n",
    "# 불러오기\n",
    "with open(load_path, \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "print(\"✅ 불러온 하이퍼파라미터:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84344ba6-ba2f-426a-b340-29cff29c49f2",
   "metadata": {},
   "source": [
    "# 코드셀 6 - 튜닝 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d731dbfd-470e-49c5-a32f-cb2102868e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n",
      "✅ 사전학습 가중치 로드 완료 (fc 제외)\n",
      "✅ YOLOv8 기반 전체 학습 구성 완료\n"
     ]
    }
   ],
   "source": [
    "# ✅ 튜닝 결과 존재 확인\n",
    "assert 'best_params' in globals(), \"Optuna 튜닝 결과가 없습니다.\"\n",
    "\n",
    "# ✅ 하이퍼파라미터 추출\n",
    "batch_size     = best_params.get('batch_size', 32)\n",
    "dropout        = best_params['dropout']\n",
    "lr             = best_params['lr']\n",
    "weight_decay   = best_params['weight_decay']\n",
    "label_smooth   = best_params.get('label_smoothing', 0.1)\n",
    "use_focal      = best_params.get('use_focal', True)\n",
    "gamma          = best_params.get('gamma', 2.0)\n",
    "\n",
    "# ✅ 클래스별 alpha 가중치 튜닝값\n",
    "alpha = [\n",
    "    best_params.get('alpha_anger',   1.5),\n",
    "    best_params.get('alpha_happy',   1.0),\n",
    "    best_params.get('alpha_panic',   1.0),\n",
    "    best_params.get('alpha_sadness', 1.3)\n",
    "]\n",
    "\n",
    "# ✅ 경로 설정\n",
    "train_dir = r\"C:\\Users\\SCK\\Desktop\\affectnet_split\\train\"\n",
    "val_dir   = r\"C:\\Users\\SCK\\Desktop\\affectnet_split\\val\"\n",
    "weight_path = r\"C:\\Users\\SCK\\Emotion_FAN\\pretrain_model\\Resnet18_FER+_pytorch.pth.tar\"\n",
    "\n",
    "# ✅ YOLOv8 얼굴 탐지기 로드\n",
    "face_detector = YOLO(\"yolov8n-face.pt\")\n",
    "\n",
    "# ✅ 데이터 로더 구성 (YOLO 기반 얼굴 감지 포함)\n",
    "train_loader, val_loader = get_data_loaders(\n",
    "    train_dir=train_dir,\n",
    "    val_dir=val_dir,\n",
    "    img_mean=img_mean,  # 전역 변수\n",
    "    img_std=img_std,    # 전역 변수\n",
    "    face_detector=face_detector,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# ✅ 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ 모델 초기화 및 백본 로드\n",
    "model = ResNet_AT_Attention(BasicBlock, [2, 2, 2, 2], num_classes=4, dropout_rate=dropout).to(device)\n",
    "load_pretrained_backbone(model, weight_path)\n",
    "\n",
    "# ✅ 학습할 레이어만 설정\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for name, module in model.named_children():\n",
    "    if name in ['layer2', 'layer3', 'layer4', 'pred_fc1', 'alpha']:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# ✅ 옵티마이저, 손실함수, 스케줄러 구성\n",
    "optimizer, criterion, scheduler = get_training_components(\n",
    "    model=model,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    label_smooth=label_smooth,\n",
    "    use_focal=use_focal,\n",
    "    gamma=gamma,\n",
    "    alpha=alpha\n",
    ")\n",
    "\n",
    "print(\"✅ YOLOv8 기반 전체 학습 구성 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d361d7f-0b57-4dad-bd48-a81809fff34e",
   "metadata": {},
   "source": [
    "# 코드셀 7 학습/검증 루프 함수 및 모델 저장 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb5c92ff-a210-443f-9e92-a91cfc5ba05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0.0, 0\n",
    "    total_samples = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=\"Train\", leave=False):\n",
    "        if len(images) == 0:  # ✅ 빈 배치 예외 처리\n",
    "            continue\n",
    "\n",
    "        images = torch.stack(images).to(device)      # ✅ tuple → tensor\n",
    "        labels = torch.tensor(labels).to(device)     # ✅ list/tuple → tensor\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        features, _ = model(images, phrase=\"eval\", AT_level=\"first_level\")\n",
    "        outputs = model(None, phrase=\"eval\", AT_level=\"pred\", vm=features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    class_f1 = f1_score(all_labels, all_preds, average=None)\n",
    "\n",
    "    return avg_loss, accuracy, macro_f1, class_f1\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0.0, 0\n",
    "    total_samples = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Val\", leave=False):\n",
    "            if len(images) == 0:  # ✅ 빈 배치 예외 처리\n",
    "                continue\n",
    "\n",
    "            images = torch.stack(images).to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            features, _ = model(images, phrase=\"eval\", AT_level=\"first_level\")\n",
    "            outputs = model(None, phrase=\"eval\", AT_level=\"pred\", vm=features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = outputs.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    class_f1 = f1_score(all_labels, all_preds, average=None)\n",
    "\n",
    "    return avg_loss, accuracy, macro_f1, class_f1\n",
    "\n",
    "\n",
    "def save_model(model, save_path):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"💾 모델 저장 완료: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a2fea-433e-46f8-bd0e-e91105db933e",
   "metadata": {},
   "source": [
    "# 코드셀 8  전체 학습 루프 실행 (Epoch 반복 + Early Stopping + 모델 저장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "482c71f0-a1e4-4bb4-aed6-b00796cb5b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌀 Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Train Loss: 0.2256 | Acc: 0.8288 | Macro F1: 0.8207\n",
      "📉 Val   Loss: 0.1375 | Acc: 0.8871 | Macro F1: 0.8838\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.8080\n",
      "  sadness: 0.9137\n",
      "  anger: 0.8433\n",
      "  panic: 0.7178\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.8639\n",
      "  sadness: 0.9437\n",
      "  anger: 0.8912\n",
      "  panic: 0.8365\n",
      "💾 모델 저장 완료: C:\\Users\\SCK\\checkpoints\\ea_emotionfan_with_attention.pth\n",
      "\n",
      "🌀 Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Train Loss: 0.1547 | Acc: 0.8678 | Macro F1: 0.8623\n",
      "📉 Val   Loss: 0.1311 | Acc: 0.8816 | Macro F1: 0.8765\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.8483\n",
      "  sadness: 0.9357\n",
      "  anger: 0.8764\n",
      "  panic: 0.7888\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.8549\n",
      "  sadness: 0.9435\n",
      "  anger: 0.8866\n",
      "  panic: 0.8211\n",
      "⏳ EarlyStopping 대기... 1/3\n",
      "\n",
      "🌀 Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Train Loss: 0.1355 | Acc: 0.8848 | Macro F1: 0.8795\n",
      "📉 Val   Loss: 0.1427 | Acc: 0.8758 | Macro F1: 0.8727\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.8652\n",
      "  sadness: 0.9455\n",
      "  anger: 0.8972\n",
      "  panic: 0.8100\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.8609\n",
      "  sadness: 0.9320\n",
      "  anger: 0.8794\n",
      "  panic: 0.8184\n",
      "⏳ EarlyStopping 대기... 2/3\n",
      "\n",
      "🌀 Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Train Loss: 0.1225 | Acc: 0.8931 | Macro F1: 0.8887\n",
      "📉 Val   Loss: 0.1359 | Acc: 0.8819 | Macro F1: 0.8723\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.8733\n",
      "  sadness: 0.9489\n",
      "  anger: 0.9008\n",
      "  panic: 0.8319\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.8699\n",
      "  sadness: 0.9543\n",
      "  anger: 0.8908\n",
      "  panic: 0.7744\n",
      "⏳ EarlyStopping 대기... 3/3\n",
      "⛔ Early stopping triggered!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# 학습 설정\n",
    "num_epochs = 10\n",
    "best_val_acc = 0.0\n",
    "save_path = r\"C:\\Users\\SCK\\checkpoints\\ea_emotionfan_with_attention.pth\"\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "labels = ['happy', 'sadness', 'anger', 'panic']\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\n🌀 Epoch {epoch}/{num_epochs}\")\n",
    "    \n",
    "    train_loss, train_acc, train_f1, train_class_f1 = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_f1, val_class_f1 = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"📈 Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Macro F1: {train_f1:.4f}\")\n",
    "    print(f\"📉 Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "    # 클래스별 F1 출력\n",
    "    print(\"📄 [클래스별 F1-score - Train]\")\n",
    "    for i, score in enumerate(train_class_f1):\n",
    "        print(f\"  {labels[i]}: {score:.4f}\")\n",
    "    \n",
    "    print(\"📄 [클래스별 F1-score - Val]\")\n",
    "    for i, score in enumerate(val_class_f1):\n",
    "        print(f\"  {labels[i]}: {score:.4f}\")\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        save_model(model, save_path)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"⏳ EarlyStopping 대기... {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⛔ Early stopping triggered!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd672d-d43e-4718-a42f-8ef824319dea",
   "metadata": {},
   "source": [
    "# 코드셀 9 - 커스텀 데이터셋 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e74d0cf3-58e3-4183-aad1-119a8c91b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 총 샘플 수: 5000\n",
      "✅ 진행률: 5000/5000 | 검출 성공: 4999\n",
      "📊 얼굴 crop 기준 정규화 정보\n",
      "Mean: tensor([0.5718, 0.4455, 0.3920])\n",
      "Std:  tensor([0.2276, 0.1906, 0.1701])\n",
      "🔍 커스텀 Mean: tensor([0.5718, 0.4455, 0.3920])\n",
      "🔍 커스텀 Std :  tensor([0.2276, 0.1906, 0.1701])\n",
      "✅ Saved custom_norm.json: {'mean': [0.5717952847480774, 0.4455448091030121, 0.39195016026496887], 'std': [0.22755862772464752, 0.19057472050189972, 0.17012467980384827]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ✅ 커스텀 데이터 경로\n",
    "custom_image_root = r\"C:\\Users\\SCK\\Desktop\\Data\\img\\train\"\n",
    "\n",
    "# ✅ 정규화 정보 계산 함수 재사용\n",
    "custom_img_mean, custom_img_std = compute_crop_mean_std(custom_image_root, face_detector=face_detector)\n",
    "\n",
    "# ✅ 출력 예시\n",
    "print(\"🔍 커스텀 Mean:\", custom_img_mean)\n",
    "print(\"🔍 커스텀 Std : \", custom_img_std)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "norm_stats = {\n",
    "    \"mean\": custom_img_mean.tolist(),\n",
    "    \"std\": custom_img_std.tolist()\n",
    "}\n",
    "\n",
    "with open(\"custom_norm.json\", \"w\") as f:\n",
    "    json.dump(norm_stats, f, indent=4)\n",
    "\n",
    "print(\"✅ Saved custom_norm.json:\", norm_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f04d7-1567-4ed6-8759-980d5be8d5ef",
   "metadata": {},
   "source": [
    "# 코드셀 10 - 커스텀 데이터 셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c9c7b50-4f49-4ed2-843d-255b08deefd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n",
      "✅ class_to_idx: {'anger': 0, 'happy': 1, 'panic': 2, 'sadness': 3}\n"
     ]
    }
   ],
   "source": [
    "# ✅ 커스텀 데이터셋 경로 설정\n",
    "custom_train_dir = r\"C:\\Users\\SCK\\Desktop\\Data\\img\\train\"\n",
    "custom_val_dir   = r\"C:\\Users\\SCK\\Desktop\\Data\\img\\val\"\n",
    "\n",
    "\n",
    "# ✅ DataLoader 불러오기 (함수 재사용)\n",
    "custom_train_loader, custom_val_loader = get_data_loaders(\n",
    "    train_dir=custom_train_dir,\n",
    "    val_dir=custom_val_dir,\n",
    "    img_mean=custom_img_mean,\n",
    "    img_std=custom_img_std,\n",
    "    face_detector=face_detector,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0705b82-4ebf-4ac6-a9c5-7effb8da073f",
   "metadata": {},
   "source": [
    "# 코드셀 11 - 모델 이어받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f26a2529-0107-4569-87af-d40b244011e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 [Fine-tuning] Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.2965 | Acc: 0.7960 | Macro F1: 0.7956\n",
      "📉 [Val]   Loss: 0.2334 | Acc: 0.8300 | Macro F1: 0.8295\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.7089\n",
      "  sadness: 0.9126\n",
      "  anger: 0.7612\n",
      "  panic: 0.7996\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7786\n",
      "  sadness: 0.9472\n",
      "  anger: 0.7692\n",
      "  panic: 0.8231\n",
      "💾 모델 저장 완료: C:\\Users\\SCK\\checkpoints\\ea_emotionfan_finetuned_custom.pth\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.2023 | Acc: 0.8534 | Macro F1: 0.8530\n",
      "📉 [Val]   Loss: 0.2747 | Acc: 0.8167 | Macro F1: 0.8161\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.7962\n",
      "  sadness: 0.9458\n",
      "  anger: 0.8229\n",
      "  panic: 0.8469\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7310\n",
      "  sadness: 0.9474\n",
      "  anger: 0.7822\n",
      "  panic: 0.8040\n",
      "⏳ Fine-tuning EarlyStopping 대기... 1/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.1686 | Acc: 0.8729 | Macro F1: 0.8727\n",
      "📉 [Val]   Loss: 0.3279 | Acc: 0.8050 | Macro F1: 0.8031\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.8195\n",
      "  sadness: 0.9547\n",
      "  anger: 0.8464\n",
      "  panic: 0.8703\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7670\n",
      "  sadness: 0.9524\n",
      "  anger: 0.7580\n",
      "  panic: 0.7351\n",
      "⏳ Fine-tuning EarlyStopping 대기... 2/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.1503 | Acc: 0.8817 | Macro F1: 0.8816\n",
      "📉 [Val]   Loss: 0.2885 | Acc: 0.8167 | Macro F1: 0.8150\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.8292\n",
      "  sadness: 0.9572\n",
      "  anger: 0.8552\n",
      "  panic: 0.8847\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7702\n",
      "  sadness: 0.9555\n",
      "  anger: 0.7867\n",
      "  panic: 0.7475\n",
      "⏳ Fine-tuning EarlyStopping 대기... 3/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.1192 | Acc: 0.9086 | Macro F1: 0.9084\n",
      "📉 [Val]   Loss: 0.2575 | Acc: 0.8325 | Macro F1: 0.8321\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.8698\n",
      "  sadness: 0.9699\n",
      "  anger: 0.8884\n",
      "  panic: 0.9055\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7761\n",
      "  sadness: 0.9510\n",
      "  anger: 0.7911\n",
      "  panic: 0.8100\n",
      "💾 모델 저장 완료: C:\\Users\\SCK\\checkpoints\\ea_emotionfan_finetuned_custom.pth\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0962 | Acc: 0.9231 | Macro F1: 0.9230\n",
      "📉 [Val]   Loss: 0.2676 | Acc: 0.8342 | Macro F1: 0.8330\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.8949\n",
      "  sadness: 0.9747\n",
      "  anger: 0.9073\n",
      "  panic: 0.9152\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7746\n",
      "  sadness: 0.9462\n",
      "  anger: 0.7993\n",
      "  panic: 0.8118\n",
      "💾 모델 저장 완료: C:\\Users\\SCK\\checkpoints\\ea_emotionfan_finetuned_custom.pth\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0896 | Acc: 0.9304 | Macro F1: 0.9304\n",
      "📉 [Val]   Loss: 0.3222 | Acc: 0.8367 | Macro F1: 0.8378\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9007\n",
      "  sadness: 0.9764\n",
      "  anger: 0.9151\n",
      "  panic: 0.9293\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7882\n",
      "  sadness: 0.9490\n",
      "  anger: 0.7928\n",
      "  panic: 0.8212\n",
      "💾 모델 저장 완료: C:\\Users\\SCK\\checkpoints\\ea_emotionfan_finetuned_custom.pth\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0751 | Acc: 0.9363 | Macro F1: 0.9362\n",
      "📉 [Val]   Loss: 0.3195 | Acc: 0.8283 | Macro F1: 0.8296\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9112\n",
      "  sadness: 0.9824\n",
      "  anger: 0.9203\n",
      "  panic: 0.9311\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7862\n",
      "  sadness: 0.9368\n",
      "  anger: 0.7924\n",
      "  panic: 0.8029\n",
      "⏳ Fine-tuning EarlyStopping 대기... 1/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0694 | Acc: 0.9445 | Macro F1: 0.9445\n",
      "📉 [Val]   Loss: 0.3502 | Acc: 0.8450 | Macro F1: 0.8437\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9195\n",
      "  sadness: 0.9826\n",
      "  anger: 0.9379\n",
      "  panic: 0.9378\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.8205\n",
      "  sadness: 0.9521\n",
      "  anger: 0.8140\n",
      "  panic: 0.7883\n",
      "💾 모델 저장 완료: C:\\Users\\SCK\\checkpoints\\ea_emotionfan_finetuned_custom.pth\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0625 | Acc: 0.9455 | Macro F1: 0.9455\n",
      "📉 [Val]   Loss: 0.3140 | Acc: 0.8367 | Macro F1: 0.8353\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9222\n",
      "  sadness: 0.9833\n",
      "  anger: 0.9320\n",
      "  panic: 0.9444\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7642\n",
      "  sadness: 0.9489\n",
      "  anger: 0.8056\n",
      "  panic: 0.8227\n",
      "⏳ Fine-tuning EarlyStopping 대기... 1/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0556 | Acc: 0.9556 | Macro F1: 0.9556\n",
      "📉 [Val]   Loss: 0.3659 | Acc: 0.8383 | Macro F1: 0.8384\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9384\n",
      "  sadness: 0.9850\n",
      "  anger: 0.9496\n",
      "  panic: 0.9495\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7859\n",
      "  sadness: 0.9554\n",
      "  anger: 0.7961\n",
      "  panic: 0.8164\n",
      "⏳ Fine-tuning EarlyStopping 대기... 2/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0513 | Acc: 0.9605 | Macro F1: 0.9605\n",
      "📉 [Val]   Loss: 0.3462 | Acc: 0.8392 | Macro F1: 0.8382\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9446\n",
      "  sadness: 0.9887\n",
      "  anger: 0.9547\n",
      "  panic: 0.9539\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7993\n",
      "  sadness: 0.9387\n",
      "  anger: 0.7898\n",
      "  panic: 0.8250\n",
      "⏳ Fine-tuning EarlyStopping 대기... 3/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0384 | Acc: 0.9663 | Macro F1: 0.9663\n",
      "📉 [Val]   Loss: 0.3371 | Acc: 0.8425 | Macro F1: 0.8417\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9528\n",
      "  sadness: 0.9913\n",
      "  anger: 0.9591\n",
      "  panic: 0.9621\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7910\n",
      "  sadness: 0.9494\n",
      "  anger: 0.8051\n",
      "  panic: 0.8213\n",
      "⏳ Fine-tuning EarlyStopping 대기... 4/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0311 | Acc: 0.9746 | Macro F1: 0.9747\n",
      "📉 [Val]   Loss: 0.3744 | Acc: 0.8458 | Macro F1: 0.8455\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9652\n",
      "  sadness: 0.9946\n",
      "  anger: 0.9687\n",
      "  panic: 0.9701\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7899\n",
      "  sadness: 0.9539\n",
      "  anger: 0.8071\n",
      "  panic: 0.8310\n",
      "💾 모델 저장 완료: C:\\Users\\SCK\\checkpoints\\ea_emotionfan_finetuned_custom.pth\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0290 | Acc: 0.9763 | Macro F1: 0.9763\n",
      "📉 [Val]   Loss: 0.3910 | Acc: 0.8400 | Macro F1: 0.8394\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9656\n",
      "  sadness: 0.9930\n",
      "  anger: 0.9710\n",
      "  panic: 0.9757\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7890\n",
      "  sadness: 0.9402\n",
      "  anger: 0.8027\n",
      "  panic: 0.8256\n",
      "⏳ Fine-tuning EarlyStopping 대기... 1/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0272 | Acc: 0.9775 | Macro F1: 0.9775\n",
      "📉 [Val]   Loss: 0.3656 | Acc: 0.8367 | Macro F1: 0.8363\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9676\n",
      "  sadness: 0.9940\n",
      "  anger: 0.9710\n",
      "  panic: 0.9773\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7778\n",
      "  sadness: 0.9521\n",
      "  anger: 0.7974\n",
      "  panic: 0.8182\n",
      "⏳ Fine-tuning EarlyStopping 대기... 2/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0303 | Acc: 0.9748 | Macro F1: 0.9748\n",
      "📉 [Val]   Loss: 0.3685 | Acc: 0.8350 | Macro F1: 0.8347\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9659\n",
      "  sadness: 0.9910\n",
      "  anger: 0.9711\n",
      "  panic: 0.9713\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7763\n",
      "  sadness: 0.9587\n",
      "  anger: 0.7891\n",
      "  panic: 0.8147\n",
      "⏳ Fine-tuning EarlyStopping 대기... 3/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0239 | Acc: 0.9820 | Macro F1: 0.9820\n",
      "📉 [Val]   Loss: 0.3783 | Acc: 0.8383 | Macro F1: 0.8380\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9746\n",
      "  sadness: 0.9940\n",
      "  anger: 0.9793\n",
      "  panic: 0.9800\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7739\n",
      "  sadness: 0.9539\n",
      "  anger: 0.8045\n",
      "  panic: 0.8196\n",
      "⏳ Fine-tuning EarlyStopping 대기... 4/5\n",
      "\n",
      "🔁 [Fine-tuning] Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [Train] Loss: 0.0210 | Acc: 0.9820 | Macro F1: 0.9820\n",
      "📉 [Val]   Loss: 0.3837 | Acc: 0.8383 | Macro F1: 0.8383\n",
      "📄 [클래스별 F1-score - Train]\n",
      "  happy: 0.9760\n",
      "  sadness: 0.9947\n",
      "  anger: 0.9770\n",
      "  panic: 0.9803\n",
      "📄 [클래스별 F1-score - Val]\n",
      "  happy: 0.7759\n",
      "  sadness: 0.9565\n",
      "  anger: 0.8019\n",
      "  panic: 0.8191\n",
      "⏳ Fine-tuning EarlyStopping 대기... 5/5\n",
      "⛔ Fine-tuning 조기 종료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# ✅ Fine-tuning 설정\n",
    "finetune_epochs = 30\n",
    "finetune_best_val_acc = 0.0\n",
    "finetune_patience = 5\n",
    "finetune_counter = 0\n",
    "\n",
    "labels = ['happy', 'sadness', 'anger', 'panic']  # 클래스별 F1 출력용 이름\n",
    "\n",
    "# 🔁 Fine-tuning Epoch Loop\n",
    "for epoch in range(1, finetune_epochs + 1):\n",
    "    print(f\"\\n🔁 [Fine-tuning] Epoch {epoch}/{finetune_epochs}\")\n",
    "\n",
    "    # ⬇ 학습 및 검증 (클래스별 F1 포함)\n",
    "    train_loss, train_acc, train_f1, train_class_f1 = train_one_epoch(\n",
    "        model, custom_train_loader, optimizer, criterion, device\n",
    "    )\n",
    "    val_loss, val_acc, val_f1, val_class_f1 = validate(\n",
    "        model, custom_val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    # ✅ 전체 성능 지표 출력\n",
    "    print(f\"📈 [Train] Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Macro F1: {train_f1:.4f}\")\n",
    "    print(f\"📉 [Val]   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "    # ✅ 클래스별 F1-score 출력\n",
    "    print(\"📄 [클래스별 F1-score - Train]\")\n",
    "    for i, score in enumerate(train_class_f1):\n",
    "        print(f\"  {labels[i]}: {score:.4f}\")\n",
    "    \n",
    "    print(\"📄 [클래스별 F1-score - Val]\")\n",
    "    for i, score in enumerate(val_class_f1):\n",
    "        print(f\"  {labels[i]}: {score:.4f}\")\n",
    "\n",
    "    # ✅ 스케줄러 업데이트 (val_acc 기준)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # ✅ 모델 저장 조건 및 EarlyStopping\n",
    "    if val_acc > finetune_best_val_acc:\n",
    "        finetune_best_val_acc = val_acc\n",
    "        finetune_counter = 0\n",
    "        save_model(model, r\"C:\\Users\\SCK\\checkpoints\\ea_emotionfan_finetuned_custom.pth\")\n",
    "    else:\n",
    "        finetune_counter += 1\n",
    "        print(f\"⏳ Fine-tuning EarlyStopping 대기... {finetune_counter}/{finetune_patience}\")\n",
    "        if finetune_counter >= finetune_patience:\n",
    "            print(\"⛔ Fine-tuning 조기 종료!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6bcb1-bd60-408a-8e7d-9de6d553a7c4",
   "metadata": {},
   "source": [
    "# 코드셀 12 - test set 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcc3d4c3-ae39-49ca-b58b-b1ede4c6b7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Inference: 100%|████████████████████████████████████████████████████████████████| 1200/1200 [02:08<00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Accuracy: 0.8367\n",
      "📊 Macro F1-score: 0.8351\n",
      "\n",
      "📄 [클래스별 F1-score]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.81      0.69      0.74       300\n",
      "       happy       0.92      0.95      0.93       300\n",
      "       panic       0.76      0.87      0.81       300\n",
      "     sadness       0.86      0.84      0.85       300\n",
      "\n",
      "    accuracy                           0.84      1200\n",
      "   macro avg       0.84      0.84      0.84      1200\n",
      "weighted avg       0.84      0.84      0.84      1200\n",
      "\n",
      "\n",
      "📝 예측 결과가 JSON 파일로 저장되었습니다: C:\\Users\\SCK\\Desktop\\test\\prediction_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "face_detector = YOLO(\"yolov8n-face.pt\")\n",
    "\n",
    "# ✅ 추론 전용 데이터셋\n",
    "class InferenceEmotionDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, face_detector=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.face_detector = face_detector\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        try:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            image = ImageOps.exif_transpose(image)\n",
    "\n",
    "            if self.face_detector:\n",
    "                img_np = np.array(image)\n",
    "                results = self.face_detector(img_np, verbose=False)[0]\n",
    "                if results.boxes is not None and len(results.boxes) > 0:\n",
    "                    boxes = results.boxes.xyxy.cpu().numpy()\n",
    "                    x1, y1, x2, y2 = map(int, max(boxes, key=lambda b: (b[2]-b[0])*(b[3]-b[1])))\n",
    "                    W, H = image.size\n",
    "                    x1, x2 = sorted((max(0, x1), min(W, x2)))\n",
    "                    y1, y2 = sorted((max(0, y1), min(H, y2)))\n",
    "                    image = image.crop((x1, y1, x2, y2))\n",
    "                else:\n",
    "                    print(f\"⚠️ 얼굴 미검출 → 원본 이미지 사용: {os.path.basename(path)}\")\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image, target  # ✅ label도 함께 반환\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 이미지 로딩 실패: {path} / {e}\")\n",
    "            return None\n",
    "\n",
    "# ✅ collate_fn: None 제거\n",
    "def collate_fn_remove_none(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return torch.empty(0), []\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# ✅ 전처리 정의 (정규화 포함)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=custom_img_mean.tolist(), std=custom_img_std.tolist())\n",
    "])\n",
    "\n",
    "# ✅ 데이터셋 및 로더\n",
    "test_root = r\"C:\\Users\\SCK\\Desktop\\test\\image\"\n",
    "test_dataset = InferenceEmotionDataset(test_root, transform=test_transform, face_detector=face_detector)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_remove_none)\n",
    "\n",
    "# ✅ 클래스 idx → 이름 매핑\n",
    "idx2emotion = {v: k for k, v in test_dataset.class_to_idx.items()}\n",
    "\n",
    "# ✅ 추론 클래스\n",
    "class EmotionFANPredictor_Attention:\n",
    "    def __init__(self, weight_path, at_type=\"self-attention\", dropout_rate=0.3):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ResNet_AT_Attention(BasicBlock, [2, 2, 2, 2],\n",
    "                                         num_classes=4,\n",
    "                                         at_type=at_type,\n",
    "                                         dropout_rate=dropout_rate).to(self.device)\n",
    "        self.load_weights(weight_path)\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_weights(self, ckpt_path):\n",
    "        ckpt = torch.load(ckpt_path, map_location=self.device)\n",
    "        state_dict = ckpt.get(\"state_dict\", ckpt)\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        self.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    def predict(self, img_tensor):\n",
    "        img_tensor = img_tensor.to(self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            f, alpha = self.model(img_tensor, phrase=\"eval\", AT_level=\"first_level\")\n",
    "            logits = self.model(None, phrase=\"eval\", AT_level=\"pred\", vm=f)\n",
    "            pred_idx = torch.argmax(logits, dim=1).item()\n",
    "            return pred_idx, alpha.cpu().item()\n",
    "\n",
    "# ✅ 모델 로드\n",
    "weight_path = r\"C:\\Users\\SCK\\checkpoints\\ea_emotionfan_finetuned_custom.pth\"\n",
    "predictor = EmotionFANPredictor_Attention(weight_path)\n",
    "\n",
    "# ✅ 추론 실행\n",
    "results = []\n",
    "y_true = []  # (옵션: 레이블이 있는 경우만 사용)\n",
    "y_pred = []\n",
    "\n",
    "for batch in tqdm(test_loader, desc=\"🔍 Inference\"):\n",
    "    if len(batch) == 0:\n",
    "        continue\n",
    "\n",
    "    img_tensor, targets = batch  # filenames → targets (정수 인덱스)\n",
    "    img_tensor = img_tensor[0]\n",
    "    target = targets[0]\n",
    "\n",
    "    pred_idx, _ = predictor.predict(img_tensor)\n",
    "    y_pred.append(pred_idx)\n",
    "    y_true.append(target)\n",
    "\n",
    "    results.append({\n",
    "        \"filename\": test_dataset.samples[len(y_true)-1][0],  # 또는 os.path.basename(...)\n",
    "        \"predicted_label\": idx2emotion[pred_idx],\n",
    "        \"true_label\": idx2emotion[target]\n",
    "    })\n",
    "\n",
    "\n",
    "# ✅ 평가 지표 출력 (정답 y_true가 있는 경우에만)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"\\n🎯 Accuracy: {acc:.4f}\")\n",
    "print(f\"📊 Macro F1-score: {macro_f1:.4f}\")\n",
    "print(\"\\n📄 [클래스별 F1-score]\")\n",
    "print(classification_report(y_true, y_pred, target_names=[idx2emotion[i] for i in sorted(idx2emotion.keys())]))\n",
    "\n",
    "# ✅ JSON 저장\n",
    "json_path = r\"C:\\Users\\SCK\\Desktop\\test\\prediction_results.json\"\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n📝 예측 결과가 JSON 파일로 저장되었습니다: {json_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
